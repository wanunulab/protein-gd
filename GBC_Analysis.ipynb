{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Parsing Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import PyPore\n",
    "from PyPore.DataTypes import *\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "from Dimer_Analysis import TraceFile\n",
    "import Dimer_Analysis \n",
    "fname=os.path.join(\"D:\\\\ProteinData\\Meni\\\\alphaHL_ GFP10D_1M KCl_2M GdnHCl_PH7.5  traces 10kHz filter\\\\175mV\",\"B091520_005_trace_10kHz.bin\")\n",
    "\n",
    "def prepareFiles(directory,inputDict,metaOutput=\"metadata.json\",verbose=False ,acceptedExts={\".bin\",\".opt\",\".abf\"},meta_format=\"json\"):\n",
    "    for root,dirs,files in os.walk(directory,topdown=True):\n",
    "        for name in files:\n",
    "            for key,value in inputDict.items():\n",
    "                if root.find(key) is not -1 and root.find(\"175\") is not -1:\n",
    "                    if os.path.splitext(name)[1]==\".bin\":\n",
    "                        value.append(os.path.join(root,name))\n",
    "PyPore.DataTypes.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Filter Derivative Parsing Algorithm###\n",
    "class MyFilterDerivativeSegmenter( parser ):\n",
    "    '''\n",
    "    This parser will segment an event using a filter-derivative method. It will\n",
    "    first apply a bessel filter at a certain cutoff to the current, then it will\n",
    "    take the derivative of that, and segment when the derivative passes a\n",
    "    threshold.\n",
    "    '''\n",
    "\n",
    "    def __init__( self, low_threshold=0.025, high_threshold=0.12, cutoff_freq=10000.,\n",
    "        sampling_freq=1.e5 ,openholder=None,openlimits=[0.8,1.2]):\n",
    "        self.low_threshold = low_threshold\n",
    "        self.high_threshold = high_threshold\n",
    "        self.cutoff_freq = cutoff_freq\n",
    "        self.sampling_freq = sampling_freq\n",
    "        self.openholder=openholder\n",
    "        self.openlimits=openlimits\n",
    "\n",
    "    def parse( self, current ):\n",
    "        '''\n",
    "        Apply the filter-derivative method to filter the ionic current.\n",
    "        '''\n",
    "        binCount=100\n",
    "        edges=np.histogram(current,np.histogram_bin_edges(current,binCount))\n",
    "        currentCpy=np.array(current,copy=True)\n",
    "        currentCpy=currentCpy[((edges[1][-1]/2)<currentCpy)]\n",
    "        upperEdges=np.histogram(currentCpy,np.histogram_bin_edges(currentCpy,int(binCount/2)))\n",
    "        i_peak=np.argmax(upperEdges[0])\n",
    "        I_0_MLE=np.mean(upperEdges[1][i_peak-1:i_peak+3])\n",
    "        # Filter the current using a first order Bessel filter twice, one in\n",
    "        # both directions to preserve phase\n",
    "        from scipy import signal\n",
    "        nyquist = self.sampling_freq / 2.\n",
    "        b, a = signal.bessel( 1, self.cutoff_freq / nyquist, btype='low', analog=0, output='ba' )\n",
    "        filtered_current = signal.filtfilt( b, a, np.array( current ).copy() )\n",
    "        #print(filtered_current)\n",
    "        #print(current)\n",
    "        # Take the derivative\n",
    "        deriv = np.abs( np.diff( filtered_current ) )\n",
    "        #purederiv = np.diff( filtered_current )\n",
    "        # Find the edges of the blocks which fulfill pass the lower threshold\n",
    "        blocks = np.where( deriv > self.low_threshold*I_0_MLE, 1, 0 )\n",
    "        block_edges = np.abs( np.diff( blocks ) )\n",
    "        tics = np.where( block_edges == 1 )[0] + 1 \n",
    "\n",
    "        # Split points are points in the each block which pass the high\n",
    "        # threshold, with a maximum of one per block \n",
    "        split_points = [0] \n",
    "\n",
    "        for start, end in zip( tics[:-1:2], tics[1::2] ): # For all pairs of edges for a block..\n",
    "            segment = deriv[ start:end ] # Save all derivatives in that block to a segment\n",
    "            segmentCurrent= filtered_current[start:end]\n",
    "            #print(np.amax(segmentCurrent), end=\" \")\n",
    "            if (np.argmax( segment ) > self.high_threshold*I_0_MLE) and (np.amax(segmentCurrent)>I_0_MLE*0.8): # If the maximum derivative in that block is above a threshold..\n",
    "                split_points = np.concatenate( ( split_points, [ start, end ] ) ) # Save the edges of the segment \n",
    "                # Now you have the edges of all transitions saved, and so the states are the current between these transitions\n",
    "        tics = np.concatenate( ( split_points, [ current.shape[0] ] ) )\n",
    "        #tics = map( int, tics )\n",
    "        #print(tics)\n",
    "        events=[ Segment( current=current[ tics[i]:tics[i+1] ], start=tics[i],duration=tics[i+1]-tics[i] ) \n",
    "                    for i in range( 0, len(tics)-1, 2 ) ]\n",
    "        currents=[]\n",
    "        currents = [event.current for event in events if event.min>I_0_MLE*self.openlimits[0] and event.max<I_0_MLE*self.openlimits[1]]\n",
    "        conCurrents=np.hstack(currents)\n",
    "        I_0 = np.mean(conCurrents)#find I_0 for the trace\n",
    "        for event in events:\n",
    "            event.I_0=I_0\n",
    "        if self.openholder is not None: \n",
    "            self.openholder.append(conCurrents)\n",
    "        return [event for event in events if event.min< I_0_MLE*0.30 and event.max<I_0_MLE*0.5]\n",
    "\n",
    "    def set_params( self ):\n",
    "        self.low_thresh = float( self.lowThreshInput.text() )\n",
    "        self.high_thresh = float( self.highThreshInput.text() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Updated so that we incorporate the real Fourier coefficients in the feature space####\n",
    "#experimentfiles={\"_ GFP10D\":[],\"_ MBP10D\":[]}\n",
    "#experimentfiles={\"_ GFP10D\":[],\"_ MBP10D\":[],\"_ MBP+GFP10D\":[]}#,\"_ MBPMBP10D\":[],\"_ MBP Nterm\":[]}\n",
    "experimentfiles={\"_ GFP10D\":[],\"_ MBP10D\":[],\"_ MBPMBP10D\":[],\"_ MBP Nterm\":[]}\n",
    "#experimentfiles={\"_ MBP10D\":[],\"_ MBPMBP10D\":[]}\n",
    "#experimentfiles={\"_ GFP10D\":[],\"_ MBP10D\":[],\"_ MBPMBP10D\":[]}\n",
    "#experimentfiles={\"_ GFP10D\":[],\"_ GFP 5minDye 10D\":[],\"_ GFP Dye 10D\":[]}\n",
    "#experimentfiles={\"_ MBP10D\":[],\"_ MBP Nterm\":[]}\n",
    "\n",
    "#experimentfiles={\"_ MBP10D\":[]}\n",
    "\n",
    "#######################################################################\n",
    "# experimentfiles={\"_ GFP10D\":[],\"_ MBP10D\":[],\n",
    "#                  \"_ MBP+GFP10D_20_80\":[], \"_ MBP+GFP10D_50_50\":[], \n",
    "#                  \"_ MBPMBP10D\":[], \"_ MBP Nterm\":[], \"_ MBP+GFP10D_40_60\":[],\n",
    "#                  \"_ MBP+GFP10D_60_40\":[], \"_ MBP+GFP10D_80_20\":[]}\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "#experimentfiles={\"_ GFP10D\":[],\"_ MBP10D\":[],\n",
    "#                 \"_ MBP+GFP10D_20_80\":[], \"_ MBP+GFP10D_50_50\":[], \"_ MBP+GFP10D_40_60\":[],\n",
    "#                 \"_ MBP+GFP10D_60_40\":[], \"_ MBP+GFP10D_80_20\":[]}\n",
    "\n",
    "\n",
    "prepareFiles(\"D:\\\\ProteinData\\\\Meni\",experimentfiles) #Directory containing all the experimental files\n",
    "experimentfilesmod={}\n",
    "experiments={}\n",
    "\n",
    "for key,value in experimentfiles.items():\n",
    "    print(key[2:],\" \")\n",
    "    experimentfilesmod[key[2:]] = value\n",
    "    experiments[key[2:]]=[]\n",
    "    for filename in value:\n",
    "        print(\" \",filename)\n",
    "\n",
    "print (experiments)\n",
    "\n",
    "import time\n",
    "t=time.time()\n",
    "for key,filenames in experimentfilesmod.items():\n",
    "    for filename in filenames:\n",
    "        experiments[key].append(TraceFile(filename))\n",
    "        experiments[key][-1].parse(parser=MyFilterDerivativeSegmenter(sampling_freq=experiments[key][-1].second,cutoff_freq=5000,low_threshold=0.010,high_threshold=0.015)) \n",
    "        experiments[key][-1].splitEvents(segmentCount=10)\n",
    "        experiments[key][-1].quantileSignal() #Taking the first quartile, median, and third quartile of each segment\n",
    "print (\"elapsed time:\",time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load event features into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#print(experiments)\n",
    "\n",
    "eventsDf=pd.DataFrame(columns=[\"type\",\"file\",\"I_0\",\"mean\",\"mean/I_0\",\"std\",\"std/I_0\",\"duration\"])\n",
    "\n",
    "for key,files in experiments.items():\n",
    "    print (key)\n",
    "    for file in files:\n",
    "        for event in file.events:\n",
    "            #print(event)\n",
    "            d={\"type\":  key,\n",
    "                             \"file\":  file.filename,\n",
    "                             \"I_0\":   event.I_0,\n",
    "                             \"mean\":  event.mean,\n",
    "                             \"mean/I_0\":event.mean/event.I_0,\n",
    "                             \"std\":event.std,\n",
    "                             \"std/I_0\":event.std/event.I_0,\n",
    "                             \"duration\":event.duration,\n",
    "                             \"event\":event,\n",
    "                             \"event_start\":event.start,\n",
    "                             \"event_end\":event.end,\n",
    "                             \"Imin\":event.min,\n",
    "                             \"Imax\":event.max,\n",
    "                             \"Imin/I_0\":event.min/event.I_0,\n",
    "                             \"Imax/I_0\":event.max/event.I_0,\n",
    "                             \"min/std\":event.min/event.std,\n",
    "                             \"min/mean\":event.min/event.mean,\n",
    "                             \"min/duration\":event.min/event.duration,\n",
    "                             \"min/max\":event.min/event.max,\n",
    "                             \"duration/std\":event.duration/event.std,\n",
    "                             \"max/std\":event.max/event.std,\n",
    "                             \"mean/std\":event.mean/event.std,\n",
    "                             \"Blockage\":round((event.I_0-event.mean)/event.I_0, 3)}#,\n",
    "                             #\"segsmean/I_0\":[seg.mean/event.I_0 for seg in event.segments],\n",
    "                             #\"segsstd/I_0\":[seg.std/event.I_0 for seg in event.segments]}\n",
    "            \n",
    "            for i in range (len(event.segments)):\n",
    "                d[\"segment{}_mean\".format(i)]=event.segments[i].mean/event.I_0\n",
    "            for i in range (len(event.segments)):\n",
    "                d[\"segment{}_std\".format(i)]=event.segments[i].std/event.I_0\n",
    "                \n",
    "            for i in range (len(event.segments)):\n",
    "                d[\"segment{}_min\".format(i)]=event.segments[i].min/event.I_0\n",
    "            for i in range (len(event.segments)):\n",
    "                d[\"segment{}_max\".format(i)]=event.segments[i].max/event.I_0\n",
    "                \n",
    "            for i in range (len(event.segments)):\n",
    "                d[\"segment{}_first_quartile\".format(i)]=event.first_quartiles[i]/event.I_0\n",
    "                d[\"segment{}_median\".format(i)]=event.medians[i]/event.I_0\n",
    "                d[\"segment{}_third_quartile\".format(i)]=event.third_quartiles[i]/event.I_0\n",
    "\n",
    "            ##########################################\n",
    "            eventsDf=eventsDf.append(d,ignore_index=True)\n",
    "\n",
    "\n",
    "print(eventsDf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "## Gradient Boosting Classifier\n",
    "\n",
    "The following scripts handle GBC model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Checking Accuracy of different ML models#####\n",
    "import sklearn as sk\n",
    "print('The scikit-learn version is {}.'.format(sk.__version__))\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "#from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "#print(len(eventsDf.loc[eventsDf[\"type\"]==\"MBP10D\"]))\n",
    "#nomixed=eventsDf.loc[eventsDf[\"type\"]==\"GFP10D\"]\n",
    "#nomixed=eventsDf.loc[eventsDf['type']!=\"MBP Nterm\"].loc[eventsDf['type']!=\"GFP10D\"]\n",
    "#nomixed=eventsDf.loc[eventsDf['type']!=\"MBP Nterm\"]\n",
    "\n",
    "nomixed=eventsDf\n",
    "\n",
    "prior_NA = len(nomixed)\n",
    "nomixed=nomixed.loc[nomixed['duration']>300e-6].loc[nomixed['duration']<20e-3] #minimum of 125 samples and maximum of 5000 samples\n",
    "#nomixed=nomixed.loc[nomixed['duration']>2e-3].loc[nomixed['duration']<20e-3] #minimum of 500 samples and maximum of 5000 samples\n",
    "\n",
    "#nomixed=nomixed.loc[nomixed['duration']>900e-6]\n",
    "\n",
    "post_NA = len(nomixed)\n",
    "removed_NA_df = nomixed.dropna()\n",
    "\n",
    "print(f\"Size of dataframe prior to filtering by duration: {prior_NA}\")\n",
    "print(f\"Size of dataframe after filtering by duration: {post_NA}\")\n",
    "print(f\"Size of dataframe after  filtering by duration and dropping NaNs from dataframe: {len(removed_NA_df)}\")\n",
    "\n",
    "#print(f\"How many events are in each file before any removal: \\n{nomixed['file'].value_counts()}\")\n",
    "\n",
    "\n",
    "#Resampling to make MBP10D and GFP10D the same size for training and testing\n",
    "#nomixed = nomixed.mask(nomixed.eq('None')).dropna()\n",
    "\n",
    "#n = len(nomixed.loc[nomixed['type']=='MBP10D'])\n",
    "n = len(nomixed.loc[nomixed['type']=='GFP10D'])\n",
    "#n = len(nomixed.loc[nomixed['type']=='MBP Nterm'])\n",
    "#n = len(nomixed.loc[nomixed['type']=='MBPMBP10D'])\n",
    "seedVal = random.randint(0, 200)\n",
    "#################################################################################\n",
    "resamp_MBP10D = nomixed.loc[nomixed[\"type\"]==\"MBP10D\"].sample(n, random_state=seedVal)\n",
    "resamp_GFP10D = nomixed.loc[nomixed[\"type\"]==\"GFP10D\"].sample(n, random_state=seedVal)\n",
    "noMixed = pd.concat([resamp_MBP10D, resamp_GFP10D])\n",
    "#################################################################################\n",
    "#file_remove = \"\\B062622_000_trace.bin\"\n",
    "#nomixed = nomixed.loc[nomixed[\"file\"]!=\"\\B062622\"].loc[nomixed[\"file\"]!=\"\\B062627\"]\n",
    "#df[df[\"A\"].str.contains(\"Hello|Britain\")]\n",
    "#resamp_MBP10D=nomixed.loc[nomixed['type']=='MBP10D']\n",
    "#print(len(resamp_MBP10D))\n",
    "#resamp_MBP10D=resamp_MBP10D[resamp_MBP10D[\"file\"].str.contains(\"\\B081920\")==True]\n",
    "#print(len(resamp_MBP10D))\n",
    "\n",
    "#resamp_MBP10D=nomixed.loc[nomixed['type']=='MBP10D'].sample(n, random_state=seedVal)\n",
    "#resamp_MBPMBP10D = nomixed.loc[nomixed[\"type\"]==\"MBPMBP10D\"].sample(n, random_state=seedVal)\n",
    "#noMixed = pd.concat([resamp_MBP10D, resamp_MBPMBP10D])\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "#resamp_NtermMPB10D = nomixed.loc[nomixed[\"type\"]==\"MBP Nterm\"].sample(n, random_state=seedVal)\n",
    "#resamp_MBPMBP10D = nomixed.loc[nomixed[\"type\"]==\"MBPMBP10D\"].sample(n, random_state=seedVal)\n",
    "#noMixed = pd.concat([resamp_NtermMPB10D, resamp_MBPMBP10D])\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "#resamp_MBP10D = nomixed.loc[nomixed[\"type\"]==\"MBP10D\"].sample(n, random_state=seedVal)\n",
    "#resamp_NtermMPB10D = nomixed.loc[nomixed[\"type\"]==\"MBP Nterm\"].sample(n, random_state=seedVal)\n",
    "#noMixed = pd.concat([resamp_MBP10D, resamp_NtermMPB10D])\n",
    "\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "#resamp_NtermMPB10D = nomixed.loc[nomixed[\"type\"]==\"MBP Nterm\"].sample(n, random_state=seedVal)\n",
    "#resamp_GFP10D = nomixed.loc[nomixed[\"type\"]==\"GFP10D\"].sample(n, random_state=seedVal)\n",
    "#noMixed = pd.concat([resamp_NtermMPB10D, resamp_GFP10D])\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "\n",
    "#resamp_MBPMBP10D = nomixed.loc[nomixed[\"type\"]==\"MBPMBP10D\"].sample(n, random_state=1)\n",
    "#resamp_GFP10D = nomixed.loc[nomixed[\"type\"]==\"GFP10D\"].sample(n, random_state=seedVal)\n",
    "#noMixed = pd.concat([resamp_MBPMBP10D, resamp_GFP10D])\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "#resamp_MBP10D = nomixed.loc[nomixed[\"type\"]==\"MBP10D\"].sample(n, random_state=10)\n",
    "#resamp_MBPMBP10D = nomixed.loc[nomixed[\"type\"]==\"MBPMBP10D\"].sample(n, random_state=10)\n",
    "#resamp_GFP10D = nomixed.loc[nomixed[\"type\"]==\"GFP10D\"].sample(n, random_state=10)\n",
    "#noMixed = pd.concat([resamp_MBP10D, resamp_MBPMBP10D, resamp_GFP10D])\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "#resamp_MBP10D = nomixed.loc[nomixed[\"type\"]==\"MBP10D\"].sample(n, random_state=seedVal)\n",
    "#resamp_MBPMBP10D = nomixed.loc[nomixed[\"type\"]==\"MBPMBP10D\"].sample(n, random_state=seedVal)\n",
    "#resamp_NtermMPB10D = nomixed.loc[nomixed[\"type\"]==\"MBP Nterm\"].sample(n, random_state=seedVal)\n",
    "#noMixed = pd.concat([resamp_MBP10D, resamp_MBPMBP10D, resamp_NtermMPB10D])\n",
    "\n",
    "#################################################################################\n",
    "#resamp_MBP10D = nomixed.loc[nomixed[\"type\"]==\"MBP10D\"].sample(n, random_state=1)\n",
    "#resamp_GFP10D = nomixed.loc[nomixed[\"type\"]==\"GFP10D\"].sample(n, random_state=1)\n",
    "#resamp_MBPMBP10D = nomixed.loc[nomixed[\"type\"]==\"MBPMBP10D\"].sample(n, random_state=1)\n",
    "#resamp_NtermMPB10D = nomixed.loc[nomixed[\"type\"]==\"MBP Nterm\"].sample(n, random_state=1)\n",
    "#noMixed = pd.concat([resamp_MBP10D, resamp_GFP10D, resamp_MBPMBP10D, resamp_NtermMPB10D])\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "#Show us how many pure samples of GFPD10 and MBPD10 events we have available for this ML analysis\n",
    "#print(f\"How many MBPD10 and GFPD10: \\n{noMixed['type'].value_counts()}\")\n",
    "#print(\" ... \\n\")\n",
    "\n",
    "print(f\"How many MBPD10 and GFPD10 and Dimer_MBPD10: \\n{nomixed['type'].value_counts()}\")\n",
    "print(\" ... \\n\")\n",
    "\n",
    "#print(f\"How many MBPD10 and GFPD10 and NtermMBPD10 and Dimer_MBPD10: \\n{nomixed['type'].value_counts()}\")\n",
    "#print(\" ... \\n\")\n",
    "\n",
    "#print(f\"How many MBPD10 and Dimer_MBPD10: \\n{nomixed['type'].value_counts()}\")\n",
    "#print(\" ... \\n\")\n",
    "\n",
    "#########################################################\n",
    "feature_space = []\n",
    "remove_fc = \"FC\"\n",
    "for (columnName, columnData) in noMixed.iteritems():\n",
    "    if remove_fc in columnName:\n",
    "        pass\n",
    "    else:\n",
    "        feature_space.append(columnName)\n",
    "\n",
    "noMixed = noMixed.loc[:, feature_space]\n",
    "\n",
    "#########################################################\n",
    "\n",
    "#Feature space preparation\n",
    "#x=nomixed.drop(columns=['event','file','I_0','Imax', 'mean', 'std', 'event_start', 'event_end', 'fc_0'])\n",
    "x=noMixed.drop(columns=['event','file','I_0','Imax', 'mean', 'std', 'event_start', 'event_end'])#, 'fc_0'])\n",
    "print(f\"How many samples in the training and testing combined dataframe: \\n{x['type'].value_counts()}\")\n",
    "print(\" ... \\n\")\n",
    "\n",
    "\n",
    "#More features are removed\n",
    "x=x.drop(columns=['type','Blockage','mean/I_0', 'duration', 'std/I_0', 'Imin', 'max/std', 'min/duration', 'duration/std'])#'Imax/I_0','min/max','Imin/I_0','min/mean','min/std', 'mean/std'])\n",
    "x=x.drop(columns=['min/max', 'min/mean', 'min/std', 'mean/std', 'Imin/I_0', 'Imax/I_0'])#'Imax/I_0','min/max','Imin/I_0','min/mean','min/std', 'mean/std'])\n",
    "\n",
    "\n",
    "#training_features = ['min/std','Imax/I_0','sm1','Imin/I_0','min/mean','sm0','ss0','sm1','ss1',\n",
    "#                     'sm2','ss2','seg0_fc4','seg1_fc4','seg2_fc4','sm4','ss4','sm5','ss5',\n",
    "#                     'sm6','ss6','seg4_fc4','seg5_fc4','seg6_fc4'] #subsequences Mean\n",
    "#x = x.loc[:, training_features]\n",
    "\n",
    "feature_space = []\n",
    "remove_fc0 = \"_fc0\"\n",
    "for (columnName, columnData) in x.iteritems():\n",
    "    if remove_fc0 in columnName:\n",
    "        pass\n",
    "    else:\n",
    "        feature_space.append(columnName)\n",
    "\n",
    "x = x.loc[:, feature_space]\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#feature_space_updated = []\n",
    "#extra_removal = [\"ss8\", \"sm3\", \"sm4\", \"ss9\", \"sm9\", \"sm6\", \"ss7\", \"ss1\"]        \n",
    "#x = x.drop(columns=extra_removal)\n",
    "########################################################################\n",
    "print(x)\n",
    "###Remove all segment FCs or Fourier coefficients###\n",
    "# feature_space = []\n",
    "# remove_fc0 = \"fc\"\n",
    "# for (columnName, columnData) in x.iteritems():\n",
    "#     if remove_fc0 in columnName:\n",
    "#         pass\n",
    "#     else:\n",
    "#         feature_space.append(columnName)\n",
    "\n",
    "# x = x.loc[:, feature_space]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################\n",
    "y=noMixed['type']\n",
    "#y=nomixed['type']\n",
    "######################\n",
    "\n",
    "features = []\n",
    "for (columnName, columnData) in x.iteritems():\n",
    "    features.append(columnName)\n",
    "\n",
    "print(f\"The following list contains the feature space used for training the model: \\n{features}\")\n",
    "\n",
    "seedVal = random.randint(0, 200)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=seedVal) #2 #40 (123 seed was good for all four analytes)\n",
    "scaler = sk.preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform( x_train )\n",
    "X_test = scaler.transform( x_test )\n",
    "print(\"...\")\n",
    "\n",
    "\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=200,max_depth=5)\n",
    "\n",
    "\n",
    "learning_rate = 0.05\n",
    "max_feature = 10\n",
    "max_depth = 3\n",
    "min_samples_split = 0.35\n",
    "min_samples_leaf = 0.35\n",
    "#from xgboost import XGBClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=500, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 max_depth=max_depth, \n",
    "                                 max_features=max_feature, \n",
    "                                 min_samples_split=min_samples_split,\n",
    "                                 min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "\n",
    "\n",
    "#rfc = RandomForestClassifier(n_estimators=200, max_depth=5)#, max_features=None)\n",
    "#logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "#knn = KNeighborsClassifier(n_neighbors=5)\n",
    "#clf = SVC(decision_function_shape='ovr',probability=True,kernel='rbf', \n",
    "#          C=200,verbose=True,gamma='scale', class_weight = None)\n",
    "\n",
    "#gbc.fit(x_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "#clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred=gbc.predict(X_test)\n",
    "#y_pred=clf.predict(x_test)\n",
    "\n",
    "#print(recall_score(y_test, y_pred, pos_label=\"MBP10D\"))\n",
    "print('...')\n",
    "print('Size of training set {}.'.format(len(x_train)))\n",
    "print('Size of testing set {}.'.format(len(x_test)))\n",
    "print('Classification accuracy of GBC model: {} and the training test split seed value: {}'.format(accuracy_score(y_test,y_pred), seedVal))\n",
    "#print('Classification accuracy of SVM model: {}'.format(accuracy_score(y_test,y_pred)))\n",
    "#print(classification_report(y_test, y_pred))\n",
    "\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)#, normalize='true' )\n",
    "print('Confusion matrix:\\n', conf_mat)\n",
    "\n",
    "feature_imp = pd.Series(gbc.feature_importances_,index=features).sort_values(ascending=False)\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(feature_imp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average meature importance over multiple model generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Looping Model multiple times to get an average results on feature importance###\n",
    "\n",
    "#type_list = [\"MBP10D\", \"GFP10D\"]\n",
    "#type_list = [\"MBP10D\", \"MBP Nterm\"]\n",
    "#type_list = [\"MBP10D\", \"MBPMBP10D\"]\n",
    "#type_list = [\"MBP Nterm\", \"MBPMBP10D\"]\n",
    "type_list = [\"MBP10D\", \"MBPMBP10D\", \"MBP Nterm\"]\n",
    "#type_list = [\"MBPMBP10D\", \"GFP10D\"]\n",
    "limiting_sample_size = n\n",
    "\n",
    "loops = 10\n",
    "\n",
    "d = {}\n",
    "for loop in range(loops):\n",
    "    seedVal = random.randint(0, 200)\n",
    "    updated_df_list = []\n",
    "    for label in type_list:\n",
    "        df = nomixed.loc[nomixed[\"type\"]==label].sample(limiting_sample_size, random_state=seedVal)\n",
    "        updated_df_list.append(df)\n",
    "    noMixed = pd.concat(updated_df_list)\n",
    "    x=noMixed.drop(columns=['event','file','I_0','Imax', 'mean', 'std', 'event_start', 'event_end'])\n",
    "    print(f\"How many samples in the training and testing combined dataframe: \\n{x['type'].value_counts()}\")\n",
    "    print(\" ... \\n\")\n",
    "    x=x.drop(columns=['type','Blockage','mean/I_0', 'duration', 'std/I_0', 'Imin', 'max/std', 'min/duration', 'duration/std'])\n",
    "    x=x.drop(columns=['min/max', 'min/mean', 'min/std', 'mean/std', 'Imin/I_0', 'Imax/I_0'])\n",
    "    x = x.loc[:, feature_space]\n",
    "    y=noMixed['type']\n",
    "    \n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=seedVal) #2 #40 (123 seed was good for all four analytes)\n",
    "    scaler = sk.preprocessing.StandardScaler()\n",
    "    X_train = scaler.fit_transform( x_train )\n",
    "    X_test = scaler.transform( x_test )\n",
    "    print(\"...\")\n",
    "    gbc = GradientBoostingClassifier(n_estimators=200,max_depth=5)\n",
    "    #gbc.fit(x_train, y_train)\n",
    "    gbc.fit(X_train, y_train)\n",
    "    #clf.fit(x_train, y_train)\n",
    "    y_pred=gbc.predict(X_test)\n",
    "    #y_pred=clf.predict(x_test)\n",
    "    #print(recall_score(y_test, y_pred, pos_label=\"MBP10D\"))\n",
    "    print('...')\n",
    "    print('Size of training set {}.'.format(len(x_train)))\n",
    "    print('Size of testing set {}.'.format(len(x_test)))\n",
    "    print('Classification accuracy of GBC model: {} and the training test split seed value: {}'.format(accuracy_score(y_test,y_pred), seedVal))\n",
    "    #print(classification_report(y_test, y_pred))\n",
    "    conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)#, normalize='true' )\n",
    "    print('Confusion matrix:\\n', conf_mat)\n",
    "    feature_imp = pd.Series(gbc.feature_importances_,index=features).sort_values(ascending=False)\n",
    "    \n",
    "    feature_imp_df = pd.DataFrame(gbc.feature_importances_,index=features)#.sort_values(ascending=False)\n",
    "    feature_imp_df.columns = ['Weight']\n",
    "    feature_imp_df['Feature'] = feature_imp_df.index\n",
    "    feature_imp_df = feature_imp_df.reset_index(drop=True)\n",
    "    #print(feature_imp_df)\n",
    "    #for seg in range(0, 10):\n",
    "    if loop == 0:\n",
    "        for seg in range(0, 10):\n",
    "            weight_list = []\n",
    "            segnum = str(seg)\n",
    "            for idx, feature in enumerate(feature_imp_df[\"Feature\"]):\n",
    "                if segnum in feature:\n",
    "                    value = feature_imp_df[\"Weight\"][idx]\n",
    "                    weight_list.append(round(value,2))\n",
    "            #print(weight_list)\n",
    "            d[f\"Segment {seg+1}\"] = weight_list\n",
    "    else:\n",
    "        for seg in range(0, 10):\n",
    "            weight_list = []\n",
    "            segnum = str(seg)\n",
    "            for idx, feature in enumerate(feature_imp_df[\"Feature\"]):\n",
    "                if segnum in feature:\n",
    "                    value = feature_imp_df[\"Weight\"][idx]\n",
    "                    weight_list.append(round(value,5))\n",
    "            #print(weight_list)\n",
    "            d[f\"Segment {seg+1}\"] = [a + b for a, b in zip(d[f\"Segment {seg+1}\"], weight_list)]\n",
    "        \n",
    "for seg in range(0, 10):\n",
    "    print(d[f\"Segment {seg+1}\"])\n",
    "    d[f\"Segment {seg+1}\"] = [round(x / loops, 2) for x in d[f\"Segment {seg+1}\"]]\n",
    "\n",
    "#l1 = [90, 7, 30, 6]\n",
    "#l2 = [8,  2, 40, 5]\n",
    "\n",
    "#new = [a+b for a, b in zip(l1, l2)]\n",
    "#d = {}\n",
    "\n",
    "training_features = [\"first quartile\", \"maximum\", \"mean\", \"median\", \"minimum\", \"standard deviation\", \"third quartile\"]\n",
    "segs_df = pd.DataFrame(d, index=training_features)\n",
    "\n",
    "segs_df = segs_df*100\n",
    "print(segs_df)\n",
    "\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "x_axis_labels = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\", \"S8\", \"S9\", \"S10\"] # labels for x-axis\n",
    "#y_axis_labels = [11,22,33,44,55,66,77,88,99,101,111,121] # labels for y-axis\n",
    "vmin = 0\n",
    "vmax = 25\n",
    "plt.figure(figsize=(10, 10), dpi = 300)\n",
    "h = sns.heatmap(segs_df,\n",
    "            xticklabels=x_axis_labels,\n",
    "            annot=True,\n",
    "            annot_kws={\"fontsize\":25},\n",
    "            vmin=vmin, \n",
    "            vmax=vmax, \n",
    "            cmap='Reds',\n",
    "            yticklabels=False)\n",
    "#plt.savefig('Features_HeatMap_MBPD10_vs_D10MBP_vs_diMBPD10_Updated.png', transparent=True)\n",
    "#plt.savefig('Features_HeatMap_MBPD10_vs_D10MBP_vs_diMBPD10_Updated.svg', transparent=True)\n",
    "\n",
    "#plt.savefig('Features_HeatMap_MBPD10_vs_GFPD10_Updated.png', transparent=True)\n",
    "#plt.savefig('Features_HeatMap_MBPD10_vs_GFPD10_Updated.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate multiple GBC models and invoke on different mixture experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Generate mutliple iterations of the model####\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from matplotlib import colors\n",
    "%matplotlib inline\n",
    "\n",
    "#nomixed=nomixed.loc[nomixed['duration']>500e-6].loc[nomixed['duration']<20e-3] #minimum of 125 samples and maximum of 5000 samples\n",
    "\n",
    "calls_dict = {'GFP10D': [], 'MBP10D': []}\n",
    "\n",
    "#############################################################################\n",
    "####50 versus 50 ratio file removal####\n",
    "#filter_file = \"B092\" #pre revision experiment 1 50 versus 50 data\n",
    "#filter_file = \"B0706\" #post revision experiment 1 50 versus 50 data\n",
    "#filter_file = \"B0708\" #post revision experiment 2 50 versus 50 data\n",
    "\n",
    "#MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_20_80']\n",
    "#MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_40_60']\n",
    "#MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_50_50']\n",
    "MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_60_40']\n",
    "#MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_80_20']\n",
    "\n",
    "#MBP_GFP_mixed = MBP_GFP_mixed[MBP_GFP_mixed['file'].str.contains(filter_file) != True]\n",
    "#print(f\"Size of mixture dataframe we are predicting on: {len(MBP_GFP_mixed)}\")\n",
    "#############################################################################\n",
    "nomixed=eventsDf\n",
    "prior_NA = len(nomixed)\n",
    "\n",
    "nomixed=nomixed.loc[nomixed['duration']>300e-6].loc[nomixed['duration']<20e-3] #minimum of 125 samples and maximum of 5000 samples\n",
    "#nomixed=nomixed.loc[nomixed['duration']>900e-6]\n",
    "\n",
    "post_NA = len(nomixed)\n",
    "removed_NA_df = nomixed.dropna()\n",
    "\n",
    "print(f\"Size of dataframe prior to filtering by duration: {prior_NA}\")\n",
    "print(f\"Size of dataframe after filtering by duration: {post_NA}\")\n",
    "print(f\"Size of dataframe after  filtering by duration and dropping NaNs from dataframe: {len(removed_NA_df)}\")\n",
    "\n",
    "#\"MBP Nterm\"\n",
    "#\"MBP10D\"\n",
    "#\"MBPMBP10D\"\n",
    "#\"GFP10D\"\n",
    "\n",
    "updated_df_list = []\n",
    "type_list = [\"MBP10D\", \"GFP10D\"]\n",
    "#type_list = [\"MBP10D\", \"MBPMBP10D\"]\n",
    "#type_list = [\"MBP10D\", \"MBP Nterm\"]\n",
    "#type_list = [\"MBP Nterm\", \"MBPMBP10D\"]\n",
    "#type_list = [\"MBP10D\", \"MBP Nterm\", \"MBPMBP10D\"]\n",
    "\n",
    "#Labels = [\"MBPD10\", \"D10MBP\", \"diMBPD10\"]\n",
    "Labels = [\"MBPD10\", \"GFPD10\"]\n",
    "\n",
    "for label in type_list:\n",
    "    df = nomixed.loc[nomixed[\"type\"] == label]\n",
    "    updated_df_list.append(df)\n",
    "noMixed = pd.concat(updated_df_list)\n",
    "noMixed['type'].value_counts()\n",
    "\n",
    "limiting_sample_size = min(noMixed['type'].value_counts())\n",
    "##limiting_sample_size = 750\n",
    "\n",
    "print(f\"Sample size of each class for training and testing: {limiting_sample_size}\")\n",
    "seedVal_sample = random.randint(0, 200)\n",
    "\n",
    "#fig=plt.figure(1,clear=True,figsize=(12,80),dpi=300)\n",
    "#fig, ax = plt.subplots(3, 3, sharex=True, sharey=True)\n",
    "\n",
    "fig = plt.figure(10,figsize=(15,15),dpi=450)\n",
    "\n",
    "classification_accuracy = []\n",
    "confusion_percents = [ [[], []], [[], []] ] #two-way matrix\n",
    "#confusion_percents = [ [[], [], []], [[], [], []], [[], [], []] ] #three-way matrix\n",
    "\n",
    "for idx, i in enumerate(range(9)):\n",
    "    updated_df_list = []\n",
    "    for label in type_list:\n",
    "        df = noMixed.loc[noMixed[\"type\"]==label].sample(limiting_sample_size, random_state=seedVal_sample)\n",
    "        updated_df_list.append(df)\n",
    "    noMixed = pd.concat(updated_df_list)\n",
    "    #########################################################\n",
    "    #MBP_model_data_indices = noMixed.loc[noMixed['type']=='MBP10D'].index.tolist()\n",
    "    #MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP10D'].drop(MBP_model_data_indices)\n",
    "    #print(f\"Size of mixture dataframe we are predicting on: {len(MBP_GFP_mixed)}\")\n",
    "    \n",
    "    #GFP_model_data_indices = noMixed.loc[noMixed['type']=='GFP10D'].index.tolist()\n",
    "    #MBP_GFP_mixed = nomixed.loc[nomixed['type']=='GFP10D'].drop(GFP_model_data_indices)\n",
    "    #print(f\"Size of mixture dataframe we are predicting on: {len(MBP_GFP_mixed)}\")\n",
    "    #########################################################\n",
    "    #noMixed['type'].value_counts()\n",
    "    x=noMixed.drop(columns=['event','file','I_0','Imax', 'mean', 'std', 'event_start', 'event_end'])#, 'fc_0'])\n",
    "    x=x.drop(columns=['type','Blockage','mean/I_0', 'duration', 'std/I_0', 'Imin', 'max/std', 'min/duration', 'duration/std'])#'Imax/I_0','min/max','Imin/I_0','min/mean','min/std', 'mean/std'])\n",
    "    x=x.drop(columns=['min/max', 'min/mean', 'min/std', 'mean/std', 'Imin/I_0', 'Imax/I_0'])#'Imax/I_0','min/max','Imin/I_0','min/mean','min/std', 'mean/std'])\n",
    "    \n",
    "#     feature_space = []\n",
    "#     #remove_fc0 = \"_fc0\"\n",
    "#     remove_fc0 = \"FC\"\n",
    "#     for (columnName, columnData) in x.iteritems():\n",
    "#         if remove_fc0 in columnName:\n",
    "#             pass\n",
    "#         else:\n",
    "#             feature_space.append(columnName)\n",
    "    x = x.loc[:, feature_space]\n",
    "    y=noMixed['type']\n",
    "    seedVal_trte_split = random.randint(0, 200)\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=seedVal_trte_split) #2 #40 (123 seed was good for all four analytes)\n",
    "    scaler = sk.preprocessing.StandardScaler()\n",
    "    x_train = scaler.fit_transform( x_train )\n",
    "    x_test = scaler.transform( x_test )\n",
    "    \n",
    "    print(f\"Expected: {y_test.value_counts()}\")\n",
    "    \n",
    "    \n",
    "    gbc = GradientBoostingClassifier(n_estimators=200,max_depth=5)\n",
    "    #rfc = RandomForestClassifier(n_estimators=200, max_depth=5)#, max_features=None)\n",
    "    #logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "    #knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    #clf = SVC(decision_function_shape='ovr',probability=True,kernel='rbf', \n",
    "               #C=200,verbose=True,gamma='scale', class_weight = None)\n",
    "\n",
    "    gbc.fit(x_train, y_train)\n",
    "    #clf.fit(x_train, y_train)\n",
    "\n",
    "    y_pred=gbc.predict(x_test)\n",
    "    unique, counts = np.unique(y_pred, return_counts=True)\n",
    "    print(f\"Predicted: {dict(zip(unique, counts))}\")\n",
    "    classification_accuracy.append(accuracy_score(y_test,y_pred)*100)\n",
    "    \n",
    "    #conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)#, normalize='true' )\n",
    "    conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred,labels=type_list,normalize=\"true\")\n",
    "    conf_mat_raw = confusion_matrix(y_true=y_test, y_pred=y_pred,labels=type_list,normalize=None)\n",
    "    print('Confusion matrix:\\n', conf_mat)\n",
    "    ax = fig.add_subplot(330+1+i)\n",
    "    ax.grid(False)\n",
    "    cax = ax.matshow(conf_mat, cmap=plt.cm.Blues,vmin=0,vmax=1)\n",
    "    for k in range(conf_mat.shape[0]):\n",
    "        for j in range(conf_mat.shape[1]):\n",
    "            confusion_percents[k][j].append(conf_mat[k, j]*100)\n",
    "            if (k == 0 and j == 0) or (k == 1 and j == 1) or (k == 2 and j == 2):\n",
    "                ax.text(x=j, y=k,s=f\"{conf_mat[k, j]*100:0.1f} %\\n\\n{conf_mat_raw[k, j]}\", va='center', ha='center', size='x-large', color='white')\n",
    "            else:\n",
    "                ax.text(x=j, y=k,s=f\"{conf_mat[k, j]*100:0.1f} %\\n\\n{conf_mat_raw[k, j]}\", va='center', ha='center', size='x-large', color='k')\n",
    "\n",
    "    ax.set_title(f\"Shuffle {i+1}, {accuracy_score(y_test,y_pred)*100:.1f} %\")\n",
    "    #plt.subplot(3,3,idx+1)\n",
    "    \n",
    "    #labels = type_list\n",
    "    labels = Labels\n",
    "    ax.xaxis.tick_bottom()\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels,rotation=\"vertical\",verticalalignment='center')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Expected\")\n",
    "    \n",
    "#fig.text(0.5, 0.04, \"Predicted\", ha='center', fontsize=\"xx-large\", weight='bold')\n",
    "#fig.text(0.04, 0.5, \"Expected\", va='center', rotation='vertical', fontsize=\"xx-large\", weight='bold')\n",
    "#fig.tight_layout()\n",
    "#fig.subplots_adjust(top=.75)\n",
    "\n",
    "#plt.savefig('All_Confusions_MBPD10_vs_GFPD10.png', transparent=True)\n",
    "#plt.savefig('All_Confusions_MBPD10_vs_GFPD10.svg', transparent=True)\n",
    "\n",
    "#plt.savefig('All_Confusions_MBPD10_vs_D10MBP_vs_diMBPD10.png', transparent=True)\n",
    "#plt.savefig('All_Confusions_MBPD10_vs_D10MBP_vs_diMBPD10.svg', transparent=True)\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "    mixed=MBP_GFP_mixed.drop(columns=['event','file','I_0','Imax', 'mean', 'std', 'event_start', 'event_end'])#, 'fc_0'])\n",
    "\n",
    "    #More features are removed\n",
    "    mixed = mixed.loc[:, feature_space]\n",
    "\n",
    "    mixed_scaled = scaler.transform(mixed)\n",
    "    mixed_exp_calls = gbc.predict(mixed_scaled)\n",
    "    #mixed_exp_calls = gbc.predict(mixed)\n",
    "    #mixed_exp_calls = clf.predict(mixed_scaled)\n",
    "    unique, counts = np.unique(mixed_exp_calls, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    \n",
    "    for call, count in zip(unique, counts):\n",
    "        calls_dict[call].append(count)\n",
    "\n",
    "print('Mean classification accuracy of GBC model: {}'.format(np.mean(classification_accuracy)))  \n",
    "###############################################################################################################################################\n",
    "confusion_percents_array = np.array(confusion_percents)\n",
    "print(confusion_percents_array)\n",
    "\n",
    "plusminus = u\"\\u00B1\"\n",
    "fig = plt.figure(11,figsize=(6,6),dpi=450)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.grid(False)\n",
    "cax = ax.matshow(conf_mat, cmap=plt.cm.Blues,vmin=0,vmax=1)\n",
    "for k in range(confusion_percents_array.shape[0]):\n",
    "    for j in range(confusion_percents_array.shape[1]):\n",
    "        if (k == 0 and j == 0) or (k == 1 and j == 1) or (k == 2 and j == 2):\n",
    "            ax.text(x=j, y=k,s=f\"{np.mean(confusion_percents_array[k, j]):0.1f}{plusminus}{np.std(confusion_percents_array[k, j]):0.2f}%\", va='center', ha='center', size='x-large', color='white')\n",
    "        else:\n",
    "            ax.text(x=j, y=k,s=f\"{np.mean(confusion_percents_array[k, j]):0.1f}{plusminus}{np.std(confusion_percents_array[k, j]):0.2f}%\", va='center', ha='center', size='x-large', color='k')\n",
    "labels = Labels\n",
    "ax.set_title(f\"Mean Classification Accuracy: {np.mean(classification_accuracy):.1f} %\")\n",
    "ax.xaxis.tick_bottom()\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels,rotation=\"vertical\",verticalalignment='center')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Expected\")\n",
    "\n",
    "#plt.savefig('Mean_Confusion_MBPD10_vs_GFPD10.png', transparent=True)\n",
    "#plt.savefig('Mean_Confusion_MBPD10_vs_GFPD10.svg', transparent=True)\n",
    "\n",
    "#plt.savefig('Mean_Confusion_MBPD10_vs_D10MBP_vs_diMBPD10.png', transparent=True)\n",
    "#plt.savefig('Mean_Confusion_MBPD10_vs_D10MBP_vs_diMBPD10.svg', transparent=True)\n",
    "###############################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "#     if i==0:\n",
    "#         single_fig=plt.figure(11,figsize=(3,3.5),dpi=450)\n",
    "#         ax=single_fig.gca().matshow(conf_mat, cmap=plt.cm.Blues,vmin=0,vmax=1)\n",
    "#         for k in range(conf_mat.shape[0]):\n",
    "#             for j in range(conf_mat.shape[1]):\n",
    "#                 plt.text(x=j, y=k,s=f\"{conf_mat[k, j]*100:0.1f} %\\n\\n{conf_mat_raw[k, j]}\", va='center', ha='center', size='x-large')\n",
    "# #                 plt.text(x=j, y=k,s=f\"{conf_mat_raw[k, j]}\", va='bottom', ha='center', size='x-large')\n",
    "#         plt.title(f\"GBC test confusion matrix\\naccuracy: {accuracy_score(y_test,y_pred)*100:.1f} %\")\n",
    "# #         accuracies.append(accuracy_score(y_test,y_pred)*100)\n",
    "#         plt.gca().xaxis.tick_bottom()\n",
    "#         plt.xlabel(\"Predicted\")\n",
    "#         plt.ylabel(\"Expected\")\n",
    "        \n",
    "#         plt.gca().set_xticklabels([''] + labels)\n",
    "#         plt.gca().set_yticklabels([''] + labels,rotation=\"vertical\",verticalalignment='center')\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        #plt.figure(10)\n",
    "        \n",
    "#fig.colorbar(cax)\n",
    "\n",
    "#fig.ylabel('Expected')\n",
    "#fig.xlabel('Predicted')\n",
    "#     plt.show()\n",
    "#fig.title('Mean classification accuracy of GBC model: {}'.format(np.mean(classification_accuracy)))\n",
    "\n",
    "#     title = f\"Normalized confusion matrix iteration: {idx+1}\"\n",
    "#     #class_names = ['GFP', 'MBP']\n",
    "#     disp = plot_confusion_matrix(gbc, x_test, y_test,\n",
    "#                                  display_labels=type_list,#class_names,\n",
    "#                                  cmap=plt.cm.binary,\n",
    "#                                  #cmap=plt.cm.RdYlGn,\n",
    "#                                  normalize='true')\n",
    "#     disp.ax_.set_title(title)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plt.savefig('MBPD10_vs_GFPD10_Confusion.png', transparent=True)\n",
    "#plt.savefig('MBPD10_vs_GFPD10_Confusion.svg', transparent=True)\n",
    "\n",
    "#plt.savefig('MBPD10_vs_MBPD10_Dimer_Confusion.png', transparent=True)\n",
    "#plt.savefig('MBPD10_vs_MBPD10_Dimer_Confusion.svg', transparent=True)\n",
    "\n",
    "#plt.savefig('MBPD10_vs_MBP Nterm_Confusion.png', transparent=True)\n",
    "#plt.savefig('MBPD10_vs_MBP Nterm_Confusion.svg', transparent=True)\n",
    "\n",
    "#plt.savefig('Dimer_vs_MBP Nterm_Confusion.png', transparent=True)\n",
    "#plt.savefig('Dimer_vs_MBP Nterm_Confusion.svg', transparent=True)\n",
    "\n",
    "#plt.savefig('MBPD10_vs_MBP Nterm_vs_MBPD10_Dimer_Confusion.png', transparent=True)\n",
    "#plt.savefig('MBPD10_vs_MBP Nterm_vs_MBPD10_Dimer_Confusion.svg', transparent=True)\n",
    "\n",
    "\n",
    "#save_results_to = 'Figures/Updated_Figures/GFP_WT_150mV_Figs/'\n",
    "#plt.savefig(save_results_to + 'MBPD10_120mV_Segmentation_4_components.png', transparent=True, dpi = 300)\n",
    "#plt.savefig(save_results_to + 'MBPD10_120mV_Segmentation_4_components.svg', transparent=True, dpi = 300)\n",
    "\n",
    "\n",
    "\n",
    "#size_0 = len(test_df.loc[test_df['type']==x1])\n",
    "#size_100perc_mod_test = len(test_df.loc[test_df['type']==x2])\n",
    "    \n",
    "#if size_0perc_mod_test <= size_100perc_mod_test:\n",
    "#    original_test_100_mod = test_df.loc[test_df['type']==x2].sample(size_0perc_mod_test)\n",
    "#    original_test_0_mod = test_df.loc[test_df['type']==x1]\n",
    "#    size_0perc_mod_test = len(original_test_0_mod)\n",
    "#    size_100perc_mod_test = len(original_test_100_mod)\n",
    "#else:\n",
    "#    original_test_0_mod = test_df.loc[test_df['type']==x1].sample(size_100perc_mod_test)\n",
    "#    original_test_100_mod = test_df.loc[test_df['type']==x2]\n",
    "#    size_0perc_mod_test = len(original_test_0_mod)\n",
    "#    size_100perc_mod_test = len(original_test_100_mod)\n",
    "    \n",
    "#print(f'Size of original X-test after balancing: 0% mod: {size_0perc_mod_test}\\n 100% mod: {size_100perc_mod_test}')\n",
    "\n",
    "\n",
    "\n",
    "# #Resampling to make MBP10D and GFP10D the same size for training and testing\n",
    "# #n = len(nomixed.loc[nomixed['type']=='GFP10D'])\n",
    "# n = len(nomixed.loc[nomixed['type']=='MBP Nterm'])\n",
    "# #n = len(nomixed.loc[nomixed['type']=='MBPMBP10D'])\n",
    "# seedVal = random.randint(0, 200)\n",
    "# #################################################################################\n",
    "# #resamp_MBP10D = nomixed.loc[nomixed[\"type\"]==\"MBP10D\"].sample(n, random_state=seedVal)\n",
    "# #resamp_GFP10D = nomixed.loc[nomixed[\"type\"]==\"GFP10D\"].sample(n, random_state=seedVal)\n",
    "# #noMixed = pd.concat([resamp_MBP10D, resamp_GFP10D])\n",
    "# #################################################################################\n",
    "\n",
    "# #resamp_MBP10D = nomixed.loc[nomixed[\"type\"]==\"MBP10D\"].sample(n, random_state=1)\n",
    "# #resamp_MBPMBP10D = nomixed.loc[nomixed[\"type\"]==\"MBPMBP10D\"].sample(n, random_state=1)\n",
    "# #noMixed = pd.concat([resamp_MBP10D, resamp_MBPMBP10D])\n",
    "\n",
    "# #################################################################################\n",
    "\n",
    "# #################################################################################\n",
    "\n",
    "# resamp_MBP10D = nomixed.loc[nomixed[\"type\"]==\"MBP10D\"].sample(n, random_state=seedVal)\n",
    "# resamp_NtermMPB10D = nomixed.loc[nomixed[\"type\"]==\"MBP Nterm\"].sample(n, random_state=seedVal)\n",
    "# noMixed = pd.concat([resamp_MBP10D, resamp_NtermMPB10D])\n",
    "\n",
    "\n",
    "# x=noMixed.drop(columns=['event','file','I_0','Imax', 'mean', 'std', 'event_start', 'event_end'])#, 'fc_0'])\n",
    "# print(f\"How many samples in the training and testing combined dataframe: \\n{x['type'].value_counts()}\")\n",
    "# print(\" ... \\n\")\n",
    "\n",
    "# #More features are removed\n",
    "# x=x.drop(columns=['type','Blockage','mean/I_0', 'duration', 'std/I_0', 'Imin', 'max/std', 'min/duration', 'duration/std'])#'Imax/I_0','min/max','Imin/I_0','min/mean','min/std', 'mean/std'])\n",
    "# x=x.drop(columns=['min/max', 'min/mean', 'min/std', 'mean/std', 'Imin/I_0', 'Imax/I_0'])#'Imax/I_0','min/max','Imin/I_0','min/mean','min/std', 'mean/std'])\n",
    "\n",
    "\n",
    "# #training_features = ['min/std','Imax/I_0','sm1','Imin/I_0','min/mean','sm0','ss0','sm1','ss1',\n",
    "# #                     'sm2','ss2','seg0_fc4','seg1_fc4','seg2_fc4','sm4','ss4','sm5','ss5',\n",
    "# #                     'sm6','ss6','seg4_fc4','seg5_fc4','seg6_fc4'] #subsequences Mean\n",
    "# #x = x.loc[:, training_features]\n",
    "\n",
    "# feature_space = []\n",
    "# remove_fc0 = \"_fc0\"\n",
    "# for (columnName, columnData) in x.iteritems():\n",
    "#     if remove_fc0 in columnName:\n",
    "#         pass\n",
    "#     else:\n",
    "#         feature_space.append(columnName)\n",
    "\n",
    "# x = x.loc[:, feature_space]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######################\n",
    "# y=noMixed['type']\n",
    "# #y=nomixed['type']\n",
    "# ######################\n",
    "\n",
    "# features = []\n",
    "# for (columnName, columnData) in x.iteritems():\n",
    "#     features.append(columnName)\n",
    "\n",
    "# print(f\"The following list contains the feature space used for training the model: \\n{features}\")\n",
    "\n",
    "# seedVal = random.randint(0, 200)\n",
    "# x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=seedVal) #2 #40 (123 seed was good for all four analytes)\n",
    "# scaler = sk.preprocessing.StandardScaler()\n",
    "# x_train = scaler.fit_transform( x_train )\n",
    "# x_test = scaler.transform( x_test )\n",
    "# print(\"...\")\n",
    "\n",
    "\n",
    "\n",
    "# gbc = GradientBoostingClassifier(n_estimators=200,max_depth=5)\n",
    "# #rfc = RandomForestClassifier(n_estimators=200, max_depth=5)#, max_features=None)\n",
    "# #logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "# #knn = KNeighborsClassifier(n_neighbors=5)\n",
    "# #clf = SVC(decision_function_shape='ovr',probability=True,kernel='rbf', \n",
    "# #          C=200,verbose=True,gamma='scale', class_weight = None)\n",
    "\n",
    "# gbc.fit(x_train, y_train)\n",
    "# #clf.fit(x_train, y_train)\n",
    "\n",
    "# y_pred=gbc.predict(x_test)\n",
    "# #y_pred=clf.predict(x_test)\n",
    "\n",
    "\n",
    "# print('...')\n",
    "# print('Size of training set {}.'.format(len(x_train)))\n",
    "# print('Size of testing set {}.'.format(len(x_test)))\n",
    "# print('Classification accuracy of GBC model: {} and the training test split seed value: {}'.format(accuracy_score(y_test,y_pred), seedVal))\n",
    "# #print('Classification accuracy of SVM model: {}'.format(accuracy_score(y_test,y_pred)))\n",
    "# #print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(calls_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Save and plot calls from ratios###\n",
    "n_20_80 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_20_80'])\n",
    "n_40_60 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_40_60'])\n",
    "n_50_50 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_50_50'])\n",
    "n_60_40 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_60_40'])\n",
    "n_80_20 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_80_20'])    \n",
    "\n",
    "#calls_dict_80_20 = calls_dict\n",
    "#print(calls_dict_80_20)\n",
    "\n",
    "#calls_dict_60_40 = calls_dict\n",
    "#print(calls_dict_60_40)\n",
    "\n",
    "#calls_dict_40_60 = calls_dict\n",
    "#print(calls_dict_40_60)\n",
    "\n",
    "#calls_dict_20_80 = calls_dict\n",
    "#print(calls_dict_20_80)\n",
    "\n",
    "#calls_dict_50_50 = calls_dict\n",
    "#print(calls_dict_50_50)\n",
    "\n",
    "#calls_dict_100_0 = calls_dict\n",
    "#print(calls_dict_100_0)\n",
    "\n",
    "calls_dict_0_100 = calls_dict\n",
    "print(calls_dict_0_100)\n",
    "\n",
    "# GFP_mean_20_80 = np.mean(np.array(calls_dict_20_80['GFP10D'])/n_20_80)\n",
    "# GFP_std_20_80 = np.std(np.array(calls_dict_20_80['GFP10D'])/n_20_80)\n",
    "# print(GFP_mean_20_80)\n",
    "# print(GFP_std_20_80)\n",
    "# GFP_mean_40_60 = np.mean(np.array(calls_dict_40_60['GFP10D'])/n_40_60)\n",
    "# GFP_std_40_60 = np.std(np.array(calls_dict_40_60['GFP10D'])/n_40_60)\n",
    "# print(GFP_mean_40_60)\n",
    "# print(GFP_std_40_60)\n",
    "# GFP_mean_50_50 = np.mean(np.array(calls_dict_50_50['GFP10D'])/n_50_50)\n",
    "# GFP_std_50_50 = np.std(np.array(calls_dict_50_50['GFP10D'])/n_50_50)\n",
    "# print(GFP_mean_50_50)\n",
    "# print(GFP_std_50_50)\n",
    "# GFP_mean_60_40 = np.mean(np.array(calls_dict_60_40['GFP10D'])/n_60_40)\n",
    "# GFP_std_60_40 = np.std(np.array(calls_dict_60_40['GFP10D'])/n_60_40)\n",
    "# print(GFP_mean_60_40)\n",
    "# print(GFP_std_60_40)\n",
    "# print(\"...\")\n",
    "# GFP_mean_80_20 = np.mean(np.array(calls_dict_80_20['GFP10D'])/n_80_20)\n",
    "# GFP_std_80_20 = np.std(np.array(calls_dict_80_20['GFP10D'])/n_80_20)\n",
    "# print(GFP_mean_80_20)\n",
    "# print(GFP_std_80_20)\n",
    "\n",
    "\n",
    "# MBP_mean_20_80 = np.mean(np.array(calls_dict_20_80['MBP10D'])/n_20_80)\n",
    "# MBP_std_20_80 = np.std(np.array(calls_dict_20_80['MBP10D'])/n_20_80)\n",
    "# print(MBP_mean_20_80)\n",
    "# print(MBP_std_20_80)\n",
    "# MBP_mean_40_60 = np.mean(np.array(calls_dict_40_60['MBP10D'])/n_40_60)\n",
    "# MBP_std_40_60 = np.std(np.array(calls_dict_40_60['MBP10D'])/n_40_60)\n",
    "# print(MBP_mean_40_60)\n",
    "# print(MBP_std_40_60)\n",
    "# MBP_mean_50_50 = np.mean(np.array(calls_dict_50_50['MBP10D'])/n_50_50)\n",
    "# MBP_std_50_50 = np.std(np.array(calls_dict_50_50['MBP10D'])/n_50_50)\n",
    "# print(MBP_mean_50_50)\n",
    "# print(MBP_std_50_50)\n",
    "# MBP_mean_60_40 = np.mean(np.array(calls_dict_60_40['MBP10D'])/n_60_40)\n",
    "# MBP_std_60_40 = np.std(np.array(calls_dict_60_40['MBP10D'])/n_60_40)\n",
    "# print(MBP_mean_60_40)\n",
    "# print(MBP_std_60_40)\n",
    "# MBP_mean_80_20 = np.mean(np.array(calls_dict_80_20['MBP10D'])/n_80_20)\n",
    "# MBP_std_80_20 = np.std(np.array(calls_dict_80_20['MBP10D'])/n_80_20)\n",
    "# print(MBP_mean_80_20)\n",
    "# print(MBP_std_80_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Save prediction results for each ratio###\n",
    "\n",
    "import pickle\n",
    "\n",
    "# with open(f\"Model_Calls/80_20_MBP_GFP_results\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(calls_dict_80_20, fp)\n",
    "    \n",
    "# with open(f\"Model_Calls/60_40_MBP_GFP_results\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(calls_dict_60_40, fp)\n",
    "    \n",
    "# with open(f\"Model_Calls/40_60_MBP_GFP_results\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(calls_dict_40_60, fp)\n",
    "    \n",
    "# with open(f\"Model_Calls/20_80_MBP_GFP_results\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(calls_dict_20_80, fp)\n",
    "\n",
    "#with open(f\"Model_Calls/100_0_MBP_GFP_results\", \"wb\") as fp:   #Pickling\n",
    "#    pickle.dump(calls_dict_100_0, fp)\n",
    "    \n",
    "# with open(f\"Model_Calls/0_100_MBP_GFP_results\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(calls_dict_0_100, fp)\n",
    "    \n",
    "    \n",
    "# with open(f\"Model_Calls/50_50_MBP_GFP_results_pre_revisions\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(calls_dict_50_50, fp)\n",
    "    \n",
    "# with open(f\"Model_Calls/50_50_MBP_GFP_results_post_revisions_1\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(calls_dict_50_50, fp)\n",
    "\n",
    "# with open(f\"Model_Calls/50_50_MBP_GFP_results_post_revisions_2\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(calls_dict_50_50, fp)\n",
    "    \n",
    "# with open(f\"Model_Calls/50_50_MBP_GFP_results_post_revisions_combined\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(calls_dict_50_50, fp)\n",
    "    \n",
    "# with open(f\"Model_Calls/50_50_MBP_GFP_results_all_combined\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(calls_dict_50_50, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Load prediction results for each ratio###\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(f\"Model_Calls/80_20_MBP_GFP_results\", \"rb\") as fp:   #Pickling\n",
    "    calls_dict_80_20 = pickle.load(fp)\n",
    "print(calls_dict_80_20)\n",
    "    \n",
    "with open(f\"Model_Calls/60_40_MBP_GFP_results\", \"rb\") as fp:   #Pickling\n",
    "    calls_dict_60_40 = pickle.load(fp)\n",
    "print(calls_dict_60_40)\n",
    "\n",
    "with open(f\"Model_Calls/40_60_MBP_GFP_results\", \"rb\") as fp:   #Pickling\n",
    "    calls_dict_40_60 = pickle.load(fp)\n",
    "print(calls_dict_40_60)\n",
    "    \n",
    "with open(f\"Model_Calls/20_80_MBP_GFP_results\", \"rb\") as fp:   #Pickling\n",
    "    calls_dict_20_80 = pickle.load(fp)\n",
    "print(calls_dict_20_80)\n",
    "    \n",
    "with open(f\"Model_Calls/50_50_MBP_GFP_results_all_combined\", \"rb\") as fp:   #Pickling\n",
    "    calls_dict_50_50 = pickle.load(fp)\n",
    "print(calls_dict_50_50) \n",
    "    \n",
    "with open(f\"Model_Calls/100_0_MBP_GFP_results\", \"rb\") as fp:   #Pickling\n",
    "    calls_dict_100_0 = pickle.load(fp)\n",
    "print(calls_dict_100_0) \n",
    "    \n",
    "with open(f\"Model_Calls/0_100_MBP_GFP_results\", \"rb\") as fp:   #Pickling\n",
    "    calls_dict_0_100 = pickle.load(fp)\n",
    "print(calls_dict_0_100)\n",
    "        \n",
    "   \n",
    "with open(f\"Model_Calls/50_50_MBP_GFP_results_pre_revisions\", \"rb\") as fp:   #Pickling\n",
    "    calls_dict_50_50_pre = pickle.load(fp)\n",
    "print(calls_dict_50_50_pre)\n",
    "\n",
    "with open(f\"Model_Calls/50_50_MBP_GFP_results_post_revisions_1\", \"rb\") as fp:   #Pickling\n",
    "    calls_dict_50_50_post1 = pickle.load(fp)\n",
    "with open(f\"Model_Calls/50_50_MBP_GFP_results_post_revisions_1\", \"rb\") as fp:   #Pickling\n",
    "    calls_dict_50_50_post2 = pickle.load(fp)\n",
    "    \n",
    "with open(f\"Model_Calls/50_50_MBP_GFP_results_post_revisions_combined\", \"rb\") as fp:   #Pickling\n",
    "    calls_dict_50_50_postcombined = pickle.load(fp)\n",
    "print(calls_dict_50_50_postcombined)\n",
    "    \n",
    "    \n",
    "n_20_80 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_20_80'])\n",
    "n_40_60 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_40_60'])\n",
    "\n",
    "n_60_40 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_60_40'])\n",
    "n_80_20 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_80_20']) \n",
    "\n",
    "n_50_50 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_50_50'])\n",
    "\n",
    "n_0_100 = calls_dict_0_100['MBP10D'][0] + calls_dict_0_100['GFP10D'][0]\n",
    "n_100_0 = calls_dict_100_0['MBP10D'][0] + calls_dict_100_0['GFP10D'][0]\n",
    "\n",
    "n_50_50_pre = calls_dict_50_50_pre['MBP10D'][0] + calls_dict_50_50_pre['GFP10D'][0]\n",
    "n_50_50_post1 = calls_dict_50_50_post1['MBP10D'][0] + calls_dict_50_50_post1['GFP10D'][0]  \n",
    "n_50_50_post2 = calls_dict_50_50_post2['MBP10D'][0] + calls_dict_50_50_post2['GFP10D'][0] \n",
    "n_50_50_postcombined = calls_dict_50_50_postcombined['MBP10D'][0] + calls_dict_50_50_postcombined['GFP10D'][0]  \n",
    "    \n",
    "MBP_mean_20_80 = np.mean(np.array(calls_dict_20_80['MBP10D'])/n_20_80)\n",
    "MBP_std_20_80 = np.std(np.array(calls_dict_20_80['MBP10D'])/n_20_80)\n",
    "\n",
    "MBP_mean_40_60 = np.mean(np.array(calls_dict_40_60['MBP10D'])/n_40_60)\n",
    "MBP_std_40_60 = np.std(np.array(calls_dict_40_60['MBP10D'])/n_40_60)\n",
    "\n",
    "MBP_mean_60_40 = np.mean(np.array(calls_dict_60_40['MBP10D'])/n_60_40)\n",
    "MBP_std_60_40 = np.std(np.array(calls_dict_60_40['MBP10D'])/n_60_40)\n",
    "\n",
    "MBP_mean_80_20 = np.mean(np.array(calls_dict_80_20['MBP10D'])/n_80_20)\n",
    "MBP_std_80_20 = np.std(np.array(calls_dict_80_20['MBP10D'])/n_80_20)\n",
    "\n",
    "MBP_mean_50_50 = np.mean(np.array(calls_dict_50_50['MBP10D'])/n_50_50)\n",
    "MBP_std_50_50 = np.std(np.array(calls_dict_50_50['MBP10D'])/n_50_50)\n",
    "\n",
    "MBP_mean_0_100 = np.mean(np.array(calls_dict_0_100['MBP10D'])/n_0_100)\n",
    "MBP_std_0_100 = np.std(np.array(calls_dict_0_100['MBP10D'])/n_0_100)\n",
    "\n",
    "MBP_mean_100_0 = np.mean(np.array(calls_dict_100_0['MBP10D'])/n_100_0)\n",
    "MBP_std_100_0 = np.std(np.array(calls_dict_100_0['MBP10D'])/n_100_0)\n",
    "#############################################################################################\n",
    "MBP_mean_50_50_pre = np.mean(np.array(calls_dict_50_50_pre['MBP10D'])/n_50_50_pre)\n",
    "MBP_std_50_50_pre = np.std(np.array(calls_dict_50_50_pre['MBP10D'])/n_50_50_pre)\n",
    "\n",
    "MBP_mean_50_50_post1 = np.mean(np.array(calls_dict_50_50_post1['MBP10D'])/n_50_50_post1)\n",
    "MBP_std_50_50_post1 = np.std(np.array(calls_dict_50_50_post1['MBP10D'])/n_50_50_post1)\n",
    "\n",
    "MBP_mean_50_50_post2 = np.mean(np.array(calls_dict_50_50_post2['MBP10D'])/n_50_50_post2)\n",
    "MBP_std_50_50_post2 = np.std(np.array(calls_dict_50_50_post2['MBP10D'])/n_50_50_post2)\n",
    "\n",
    "MBP_mean_50_50_postcombined = np.mean(np.array(calls_dict_50_50_postcombined['MBP10D'])/n_50_50_postcombined)\n",
    "MBP_std_50_50_postcombined = np.std(np.array(calls_dict_50_50_postcombined['MBP10D'])/n_50_50_postcombined)\n",
    "\n",
    "\n",
    "\n",
    "GFP_mean_20_80 = np.mean(np.array(calls_dict_20_80['GFP10D'])/n_20_80)\n",
    "GFP_std_20_80 = np.std(np.array(calls_dict_20_80['GFP10D'])/n_20_80)\n",
    "\n",
    "GFP_mean_40_60 = np.mean(np.array(calls_dict_40_60['GFP10D'])/n_40_60)\n",
    "GFP_std_40_60 = np.std(np.array(calls_dict_40_60['GFP10D'])/n_40_60)\n",
    "\n",
    "GFP_mean_60_40 = np.mean(np.array(calls_dict_60_40['GFP10D'])/n_60_40)\n",
    "GFP_std_60_40 = np.std(np.array(calls_dict_60_40['GFP10D'])/n_60_40)\n",
    "\n",
    "GFP_mean_80_20 = np.mean(np.array(calls_dict_80_20['GFP10D'])/n_80_20)\n",
    "GFP_std_80_20 = np.std(np.array(calls_dict_80_20['GFP10D'])/n_80_20)\n",
    "\n",
    "GFP_mean_50_50 = np.mean(np.array(calls_dict_50_50['GFP10D'])/n_50_50)\n",
    "GFP_std_50_50 = np.std(np.array(calls_dict_50_50['GFP10D'])/n_50_50)\n",
    "#############################################################################################\n",
    "GFP_mean_50_50_pre = np.mean(np.array(calls_dict_50_50_pre['GFP10D'])/n_50_50_pre)\n",
    "GFP_std_50_50_pre = np.std(np.array(calls_dict_50_50_pre['GFP10D'])/n_50_50_pre)\n",
    "\n",
    "GFP_mean_50_50_post1 = np.mean(np.array(calls_dict_50_50_post1['GFP10D'])/n_50_50_post1)\n",
    "GFP_std_50_50_post1 = np.std(np.array(calls_dict_50_50_post1['GFP10D'])/n_50_50_post1)\n",
    "\n",
    "GFP_mean_50_50_post2 = np.mean(np.array(calls_dict_50_50_post2['GFP10D'])/n_50_50_post2)\n",
    "GFP_std_50_50_post2 = np.std(np.array(calls_dict_50_50_post2['GFP10D'])/n_50_50_post2)\n",
    "\n",
    "GFP_mean_50_50_postcombined = np.mean(np.array(calls_dict_50_50_postcombined['GFP10D'])/n_50_50_postcombined)\n",
    "GFP_std_50_50_postcombined = np.std(np.array(calls_dict_50_50_postcombined['GFP10D'])/n_50_50_postcombined)\n",
    "\n",
    "GFP_mean_0_100 = np.mean(np.array(calls_dict_0_100['GFP10D'])/n_0_100)\n",
    "GFP_std_0_100 = np.std(np.array(calls_dict_0_100['GFP10D'])/n_0_100)\n",
    "\n",
    "GFP_mean_100_0 = np.mean(np.array(calls_dict_100_0['GFP10D'])/n_100_0)\n",
    "GFP_std_100_0 = np.std(np.array(calls_dict_100_0['GFP10D'])/n_100_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Plotting MBP-GFP ratio calls###\n",
    "\n",
    "\n",
    "####Updated Figure 3 that has all 4 genes in one plot####\n",
    "\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.font_manager\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import pickle\n",
    "\n",
    "sns.set(style='white')\n",
    "    \n",
    "\n",
    "# n_20_80 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_20_80'])\n",
    "# n_40_60 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_40_60'])\n",
    "# n_50_50 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_50_50'])\n",
    "# n_60_40 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_60_40'])\n",
    "# n_80_20 = len(nomixed.loc[nomixed['type']=='MBP+GFP10D_80_20']) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8),dpi=300)\n",
    "\n",
    "#x_ticks = range(0,6,1)\n",
    "x_ticks = np.arange(0,11,1)\n",
    "ax.set_xticks(x_ticks)\n",
    "\n",
    "ax.set_xticklabels(['0%','10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%'],\n",
    "                    fontdict = {'fontsize' : 15})\n",
    "\n",
    "\n",
    "# ax.set_xticklabels([f'0-100 MBP:GFP\\n(n={n_0_100})\\n{round(MBP_mean_0_100, 2)}:{round(GFP_mean_0_100, 2)}',\n",
    "#                     f'20-80 MBP:GFP\\n(n={n_20_80})\\n{round(MBP_mean_20_80, 2)}:{round(GFP_mean_20_80, 2)}',\n",
    "#                     f'30-70 MBP:GFP',\n",
    "#                     f'40-60 MBP:GFP\\n(n={n_40_60})\\n{round(MBP_mean_40_60, 2)}:{round(GFP_mean_40_60, 2)}', \n",
    "#                     f'50-50 MBP:GFP\\n(n={n_50_50})\\n{round(MBP_mean_50_50, 2)}:{round(GFP_mean_50_50, 2)}', \n",
    "#                     f'60-40 MBP:GFP\\n(n={n_60_40})\\n{round(MBP_mean_60_40, 2)}:{round(GFP_mean_60_40, 2)}',\n",
    "#                     f'70-30 GFP:MBP',\n",
    "#                     f'80-20 MBP:GFP\\n(n={n_80_20})\\n{round(MBP_mean_80_20, 2)}:{round(GFP_mean_80_20, 2)}',\n",
    "#                     f'100-0 MBP:GFP\\n(n={n_100_0})\\n{round(MBP_mean_100_0, 2)}:{round(GFP_mean_100_0, 2)}'],\n",
    "#                     fontdict = {'fontsize' : 7})#, fontdict\n",
    "\n",
    "# ax.set_xticklabels([f'PSMB2\\n({PSMB2_KMER})', f'MCM5\\n({MCM5_KMER})', f'PRPSAP1\\n({PRPSAP1_KMER})', \n",
    "#                     f'MRPS14\\n({MRPS14_KMER})', f'PTTG1IP\\n({PTTG1IP_KMER})', f'RNF7\\n({RNF7_KMER})'], fontdict = {'fontsize' : 16})#, fontdict = {'fontsize' : 7})\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xlim([-0.5, 10.5])\n",
    "\n",
    "\n",
    "\n",
    "#ax.set_ylabel(f'$\\psi$ called by model (%)', fontdict = {'fontsize' : 22})\n",
    "ax.set_ylabel(f'Predicted Ratio (GBC Classification)', fontdict = {'fontsize' : 22})\n",
    "ax.set_xlabel(f'Actual Ratio (%)', fontdict = {'fontsize' : 22})\n",
    "\n",
    "#y_ticks = np.arange(0.,1.1,0.1)\n",
    "y_ticks = np.arange(0.,1.1,0.2)\n",
    "ax.set_yticks(y_ticks)\n",
    "\n",
    "# ax.set_yticklabels([\"0\", \"0.1\", \"0.2\", \"0.3\", \n",
    "#           \"0.4\", \"0.5\", \"0.6\", \"0.7\", \n",
    "#           \"0.8\", \"0.9\", \"1\"], fontdict = {'fontsize' : 20})\n",
    "\n",
    "ax.set_yticklabels([\"0%\", \"20%\", \"40%\", \"60%\", \"80%\", \"100%\"], fontdict = {'fontsize' : 15})\n",
    "\n",
    "\n",
    "# ax.set_yticklabels([\"0\", \"10\", \"20\", \"30\", \n",
    "#           \"40\", \"50\", \"60\", \"70\", \n",
    "#           \"80\", \"90\", \"100\"], fontdict = {'fontsize' : 22})\n",
    "\n",
    "\n",
    "ax.yaxis.set_ticks_position('both')\n",
    "ax.xaxis.set_ticks_position('both')\n",
    "#ax1.tick_params(axis=\"y\",direction=\"in\", pad=5)\n",
    "#ax1.tick_params(axis=\"x\",direction=\"in\", pad=5)\n",
    "  \n",
    "    \n",
    "ax.tick_params(axis=\"y\",direction=\"in\", pad=5)\n",
    "ax.tick_params(axis=\"x\",direction=\"in\", pad=5)\n",
    "minor_locator_x = AutoMinorLocator(2)\n",
    "minor_locator_y = AutoMinorLocator(2)\n",
    "ax.tick_params(which='minor', length=2.5, width=0.5, direction='in')\n",
    "ax.xaxis.set_minor_locator(minor_locator_x)\n",
    "ax.yaxis.set_minor_locator(minor_locator_y)\n",
    "ax.grid(False)\n",
    "\n",
    "violin_parts_0_100_MBP = plt.errorbar(0, MBP_mean_0_100, yerr=MBP_std_0_100, fmt='o', solid_capstyle='projecting', \n",
    "                                   capsize=7, color='red', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"MBP\")  \n",
    "violin_parts_0_100_GFP = plt.errorbar(0, GFP_mean_0_100, yerr=GFP_std_0_100, fmt='o', solid_capstyle='projecting', \n",
    "                               capsize=7, color='green', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"GFP\")\n",
    "\n",
    "violin_parts_20_80_MBP = plt.errorbar(2, MBP_mean_20_80, yerr=MBP_std_20_80, fmt='o', solid_capstyle='projecting', \n",
    "                                   capsize=7, color='red', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"MBP\")  \n",
    "violin_parts_20_80_GFP = plt.errorbar(2, GFP_mean_20_80, yerr=GFP_std_20_80, fmt='o', solid_capstyle='projecting', \n",
    "                               capsize=7, color='green', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"GFP\")\n",
    "\n",
    "violin_parts_40_60_MBP = plt.errorbar(4, MBP_mean_40_60, yerr=MBP_std_40_60, fmt='o', solid_capstyle='projecting', \n",
    "                                   capsize=7, color='red', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"MBP\")  \n",
    "violin_parts_40_60_GFP = plt.errorbar(4, GFP_mean_40_60, yerr=GFP_std_40_60, fmt='o', solid_capstyle='projecting', \n",
    "                               capsize=7, color='green', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"GFP\")\n",
    "\n",
    "violin_parts_50_50_MBP = plt.errorbar(5, MBP_mean_50_50, yerr=MBP_std_50_50, fmt='o', solid_capstyle='projecting', \n",
    "                                   capsize=7, color='red', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"MBP\")  \n",
    "violin_parts_50_50_GFP = plt.errorbar(5, GFP_mean_50_50, yerr=GFP_std_50_50, fmt='o', solid_capstyle='projecting', \n",
    "                               capsize=7, color='green', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"GFP\")\n",
    "\n",
    "violin_parts_60_40_MBP = plt.errorbar(6, MBP_mean_60_40, yerr=MBP_std_60_40, fmt='o', solid_capstyle='projecting', \n",
    "                                   capsize=7, color='red', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"MBP\")  \n",
    "violin_parts_60_40_GFP = plt.errorbar(6, GFP_mean_60_40, yerr=GFP_std_60_40, fmt='o', solid_capstyle='projecting', \n",
    "                               capsize=7, color='green', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"GFP\")\n",
    "\n",
    "violin_parts_80_20_MBP = plt.errorbar(8, MBP_mean_80_20, yerr=MBP_std_80_20, fmt='o', solid_capstyle='projecting', \n",
    "                                   capsize=7, color='red', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"MBP\")  \n",
    "violin_parts_80_20_GFP = plt.errorbar(8, GFP_mean_80_20, yerr=GFP_std_80_20, fmt='o', solid_capstyle='projecting', \n",
    "                               capsize=7, color='green', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"GFP\")\n",
    "\n",
    "violin_parts_100_0_MBP = plt.errorbar(10, MBP_mean_100_0, yerr=MBP_std_100_0, fmt='o', solid_capstyle='projecting', \n",
    "                                   capsize=7, color='red', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"MBP\")  \n",
    "violin_parts_100_0_GFP = plt.errorbar(10, GFP_mean_100_0, yerr=GFP_std_100_0, fmt='o', solid_capstyle='projecting', \n",
    "                               capsize=7, color='green', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"GFP\")\n",
    "       \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "violin_parts_50_50_MBP_pre = plt.errorbar(5, MBP_mean_50_50_pre, yerr=MBP_std_50_50_pre, fmt='X', solid_capstyle='projecting', \n",
    "                                   capsize=7, color='red', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"MBP\", alpha=0.33)  \n",
    "violin_parts_50_50_GFP_pre = plt.errorbar(5, GFP_mean_50_50_pre, yerr=GFP_std_50_50_pre, fmt='X', solid_capstyle='projecting', \n",
    "                                   capsize=7, color='green', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"GFP\", alpha=0.33)\n",
    "\n",
    "# violin_parts_50_50_MBP_post1 = plt.errorbar(3-0.25, MBP_mean_50_50_post1, yerr=MBP_std_50_50_post1, fmt='s', solid_capstyle='projecting', \n",
    "#                                    capsize=7, color='red', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"MBP\")  \n",
    "# violin_parts_50_50_GFP_post1 = plt.errorbar(3+0.25, GFP_mean_50_50_post1, yerr=GFP_std_50_50_post1, fmt='s', solid_capstyle='projecting', \n",
    "#                                capsize=7, color='green', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"GFP\")\n",
    "\n",
    "# violin_parts_50_50_MBP_post2 = plt.errorbar(3-0.25, MBP_mean_50_50_post2, yerr=MBP_std_50_50_post2, fmt='D', solid_capstyle='projecting', \n",
    "#                                    capsize=7, color='red', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"MBP\")  \n",
    "# violin_parts_50_50_GFP_post2 = plt.errorbar(3+0.25, GFP_mean_50_50_post2, yerr=GFP_std_50_50_post2, fmt='D', solid_capstyle='projecting', \n",
    "#                                capsize=7, color='green', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"GFP\")\n",
    "\n",
    "violin_parts_50_50_MBP_postcombined = plt.errorbar(5, MBP_mean_50_50_postcombined, yerr=MBP_std_50_50_postcombined, fmt='P', solid_capstyle='projecting', \n",
    "                                   capsize=7, color='red', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"MBP\", alpha=0.33)  \n",
    "violin_parts_50_50_GFP_postcombined = plt.errorbar(5, GFP_mean_50_50_postcombined, yerr=GFP_std_50_50_postcombined, fmt='P', solid_capstyle='projecting', \n",
    "                               capsize=7, color='green', linestyle=None, lw=1, markersize=7, elinewidth=1, label=f\"GFP\", alpha=0.33)\n",
    "    \n",
    "\n",
    "    \n",
    "#vdir1_patch = mpatches.Patch(color='goldenrod', alpha=0.33, label=f\"Direct 1 Psi Calls\")#label=f\"mu: {mean2}\")\n",
    "#vdir2_patch = mpatches.Patch(color='brown', alpha=0.33, label=f\"Direct 2 Psi Calls\")#label=f\"mu: {mean2}\")\n",
    "#vdir3_patch = mpatches.Patch(color='purple', alpha=0.33, label=f\"Direct 3 Psi Calls\")#label=f\"mu: {mean2}\")\n",
    "#v_IVT_patch = mpatches.Patch(color='r', alpha=0.33, label=f\"IVT Psi Calls (FP)\")#label=f\"mu: {mean2}\")\n",
    "\n",
    "ax.legend(handles=[violin_parts_40_60_MBP, violin_parts_40_60_GFP], loc='upper left', prop={'size': 10})\n",
    "\n",
    "#ax.legend(handles=[vdir1_patch, vdir2_patch, vdir3_patch], loc='upper right', prop={'size': 9})\n",
    "#ax.legend(handles=[violin_parts_dir1_psi, violin_parts_dir2_psi, violin_parts_dir3_psi], loc='upper right', prop={'size': 12})\n",
    "\n",
    " \n",
    "##### for idx in x_ticks[1:]:\n",
    "#####     #print(\"...\" + str(samp))\n",
    "#####     plt.vlines(idx-0.5, 0, 1, color='k', linestyle='-.', lw=0.75 )\n",
    "##### #plt.grid(axis=\"y\")\n",
    "\n",
    "# plt.savefig('MBPD10_vs_GFPD10_Ratios_Total_Updated_combined_pre_and_post_revs.png')#, transparent=True)\n",
    "# plt.savefig('MBPD10_vs_GFPD10_Ratios_Total_Updated_combined_pre_and_post_revs.svg', transparent=True)\n",
    "\n",
    "\n",
    "#save_results_to = 'Figures/Figure3/'\n",
    "# plt.savefig(save_results_to + f'All_4_Dir_Results_D123.png', transparent=True, dpi = 300)\n",
    "# plt.savefig(save_results_to + f'All_4_Dir_Results_D123.svg', transparent=True, dpi = 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Plotting mixture calls with probability scores####\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "import matplotlib as mpl\n",
    "import joblib\n",
    "import matplotlib.ticker as mticker\n",
    "sns.set()\n",
    "sns.set_style(style='white')\n",
    "\n",
    "#%matplotlib qt5\n",
    "%matplotlib inline\n",
    "\n",
    "calls_dict = {'GFP10D': [], 'MBP10D': []}\n",
    "\n",
    "#############################################################################\n",
    "#filter_file = \"B071222\"\n",
    "\n",
    "#MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_20_80']\n",
    "#MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_40_60']\n",
    "MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_50_50']\n",
    "#MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_60_40']\n",
    "#MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_80_20']\n",
    "\n",
    "#MBP_GFP_mixed = MBP_GFP_mixed[MBP_GFP_mixed['file'].str.contains(filter_file)!=True]\n",
    "print(f\"Size of mixture dataframe we are predicting on: {len(MBP_GFP_mixed)}\")\n",
    "#############################################################################\n",
    "nomixed=eventsDf\n",
    "prior_NA = len(nomixed)\n",
    "\n",
    "nomixed=nomixed.loc[nomixed['duration']>300e-6].loc[nomixed['duration']<20e-3] #minimum of 125 samples and maximum of 5000 samples\n",
    "#nomixed=nomixed.loc[nomixed['duration']>900e-6]\n",
    "\n",
    "post_NA = len(nomixed)\n",
    "removed_NA_df = nomixed.dropna()\n",
    "\n",
    "print(f\"Size of dataframe prior to filtering by duration: {prior_NA}\")\n",
    "print(f\"Size of dataframe after filtering by duration: {post_NA}\")\n",
    "print(f\"Size of dataframe after  filtering by duration and dropping NaNs from dataframe: {len(removed_NA_df)}\")\n",
    "\n",
    "updated_df_list = []\n",
    "type_list = [\"MBP10D\", \"GFP10D\"]\n",
    "#type_list = [\"MBP10D\", \"MBPMBP10D\"]\n",
    "#type_list = [\"MBP10D\", \"MBP Nterm\"]\n",
    "#type_list = [\"MBP Nterm\", \"MBPMBP10D\"]\n",
    "#type_list = [\"MBP10D\", \"MBP Nterm\", \"MBPMBP10D\"]\n",
    "\n",
    "#Labels = [\"MBPD10\", \"D10MBP\", \"diMBPD10\"]\n",
    "Labels = [\"MBPD10\", \"GFPD10\"]\n",
    "\n",
    "for label in type_list:\n",
    "    df = nomixed.loc[nomixed[\"type\"] == label]\n",
    "    updated_df_list.append(df)\n",
    "noMixed = pd.concat(updated_df_list)\n",
    "noMixed['type'].value_counts()\n",
    "limiting_sample_size = min(noMixed['type'].value_counts())\n",
    "\n",
    "print(f\"Sample size of each class for training and testing: {limiting_sample_size}\")\n",
    "seedVal_sample = random.randint(0, 200)\n",
    "\n",
    "updated_df_list = []\n",
    "for label in type_list:\n",
    "    df = noMixed.loc[noMixed[\"type\"]==label].sample(limiting_sample_size, random_state=seedVal_sample)\n",
    "    updated_df_list.append(df)\n",
    "noMixed = pd.concat(updated_df_list)\n",
    "    #noMixed['type'].value_counts()\n",
    "print(f\"Total size of training and testing data: {noMixed['type'].value_counts()}\")\n",
    "\n",
    "\n",
    "x=noMixed.drop(columns=['event','file','I_0','Imax', 'mean', 'std', 'event_start', 'event_end'])#, 'fc_0'])\n",
    "x=x.drop(columns=['type','Blockage','mean/I_0', 'duration', 'std/I_0', 'Imin', 'max/std', 'min/duration', 'duration/std'])#'Imax/I_0','min/max','Imin/I_0','min/mean','min/std', 'mean/std'])\n",
    "x=x.drop(columns=['min/max', 'min/mean', 'min/std', 'mean/std', 'Imin/I_0', 'Imax/I_0'])#'Imax/I_0','min/max','Imin/I_0','min/mean','min/std', 'mean/std'])\n",
    "\n",
    "print(f\"Feature space for training model: {feature_space}\")\n",
    "x = x.loc[:, feature_space]\n",
    "y=noMixed['type']\n",
    "seedVal_trte_split = random.randint(0, 200)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=seedVal_trte_split) #2 #40 (123 seed was good for all four analytes)\n",
    "scaler = sk.preprocessing.StandardScaler()\n",
    "x_train = scaler.fit_transform( x_train )\n",
    "x_test = scaler.transform( x_test )\n",
    "    \n",
    "print(f\"Expected: {y_test.value_counts()}\")\n",
    "        \n",
    "gbc = GradientBoostingClassifier(n_estimators=200,max_depth=5)\n",
    "#rfc = RandomForestClassifier(n_estimators=200, max_depth=5)#, max_features=None)\n",
    "#logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "#knn = KNeighborsClassifier(n_neighbors=5)\n",
    "#clf = SVC(decision_function_shape='ovr',probability=True,kernel='rbf', \n",
    "           #C=200,verbose=True,gamma='scale', class_weight = None)\n",
    "\n",
    "gbc.fit(x_train, y_train)\n",
    "\n",
    "y_pred=gbc.predict(x_test)\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "print(f\"Predicted: {dict(zip(unique, counts))}\")\n",
    "\n",
    "classification_accuracy.append(accuracy_score(y_test,y_pred)*100)\n",
    "print('Mean classification accuracy of GBC model: {}'.format(np.mean(classification_accuracy)))  \n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "mixed=MBP_GFP_mixed.drop(columns=['event','file','I_0','Imax', 'mean', 'std', 'event_start', 'event_end'])#, 'fc_0'])\n",
    "\n",
    "#More features are removed\n",
    "mixed = mixed.loc[:, feature_space]\n",
    "\n",
    "mixed_scaled = scaler.transform(mixed)\n",
    "mixed_pred = gbc.predict(mixed_scaled)\n",
    "#mixed_exp_calls = gbc.predict(mixed)\n",
    "#mixed_exp_calls = clf.predict(mixed_scaled)\n",
    "unique, counts = np.unique(mixed_exp_calls, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "probability_scores = gbc.predict_proba(mixed_scaled)\n",
    "##Probs = pd.DataFrame(probs_2, columns = ['GFP10D', 'MBP10D'])\n",
    "Probs = pd.DataFrame(probability_scores, columns = ['GFP10D', 'MBP10D'])\n",
    "print(Probs['GFP10D'])\n",
    "print(Probs['MBP10D'])\n",
    "\n",
    "\n",
    "\n",
    "#print(probability_scores)\n",
    "#probs_2 = clf_2.predict_proba(X_norm_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Prepare mixture dataset pandas dataframe\n",
    "#mixed=eventsDf.loc[eventsDf['type']!=\"GFP10D\"].loc[eventsDf['type']!=\"MBP10D\"].loc[eventsDf['type']!=\"MBP Nterm\"].loc[eventsDf['type']!=\"MBPMBP10D\"].loc[eventsDf['type']!=\"GFP Dye 10D\"]\n",
    "##mixed=eventsDf.loc[eventsDf['type']!=\"GFP10D\"].loc[eventsDf['type']!=\"MBP10D\"]\n",
    "##mixed=mixed.loc[mixed['duration']>500e-6].loc[mixed['duration']<20e-3]\n",
    "##print(mixed['type'].value_counts())\n",
    "\n",
    "#Remove features that were not used for SVM training, keeping the vectors of interest\n",
    "##x_2=mixed.drop(columns=['type','event','file','I_0','Imax', 'mean', 'std', 'event_start', 'event_end'])\n",
    "##x_2=x_2.drop(columns=['Blockage','mean/I_0', 'duration', 'std/I_0', 'Imin', 'max/std', 'min/duration', 'duration/std'])\n",
    "\n",
    "\n",
    "#Normalize the data associated with each vector column-wise\n",
    "#x_norm_2 = sk.preprocessing.StandardScaler().fit_transform(x_2)\n",
    "#x_norm_2 = sk.preprocessing.MinMaxScaler(feature_range = (-1, 1)).fit_transform(x_2)\n",
    "#X_norm_2 = pd.DataFrame(x_norm_2)\n",
    "\n",
    "# scaler = sk.preprocessing.StandardScaler()\n",
    "# x_norm_2 = scaler.fit_transform( x_2 )\n",
    "# X_norm_2 = pd.DataFrame(x_norm_2)\n",
    "\n",
    "##X_norm_2 = scaler.transform( x_2 )\n",
    "#save trained model\n",
    "##joblib.dump(clf, \"MBP_GFP_Model.pkl\") \n",
    "\n",
    "#load trained model and test on unlabeled mixture\n",
    "##clf_2 = joblib.load(\"MBP_GFP_Model.pkl\")\n",
    "\n",
    "#Extract probability and decision function output from model predictions\n",
    "##mixed_pred = clf_2.predict(X_norm_2)\n",
    "#Let's observe the how many events were classified as GFP and MBP in the mixed experiments\n",
    "##unique, counts = np.unique(mixed_pred, return_counts=True)\n",
    "##print(dict(zip(unique, counts)))\n",
    "##probs_2 = clf_2.predict_proba(X_norm_2)\n",
    "##dec_2 = clf_2.decision_function(X_norm_2)\n",
    "\n",
    "\n",
    "\n",
    "#Set up dataframe containing the probability confidence scores \n",
    "##Probs = pd.DataFrame(probability_scores, columns = ['GFP10D', 'MBP10D'])\n",
    "#print(Probs['GFP10D'])\n",
    "#print(Probs['MBP10D'])\n",
    "\n",
    "#Load one of the mixed sample files for event classification\n",
    "##file = experiments[\"MBP+GFP10D\"][1]\n",
    "\n",
    "#####################################################################################################\n",
    "#file = experiments[\"MBP+GFP10D_80_20\"][10]\n",
    "#file = experiments[\"MBP+GFP10D_20_80\"][0]\n",
    "#file = experiments[\"MBP+GFP10D_50_50\"][25]\n",
    "file = experiments[\"MBP+GFP10D_50_50\"][28]\n",
    "#####################################################################################################\n",
    "\n",
    "#Let's make a new dataframe that now has the classifications associated with each event\n",
    "mixed_classified = mixed\n",
    "mixed_classified ['type'] = mixed_pred\n",
    "mixed_classified [\"pGFP\"] = probability_scores[:,0]\n",
    "mixed_classified [\"pMBP\"] = probability_scores[:,1]\n",
    "mixed_classified [\"file\"] = MBP_GFP_mixed['file']\n",
    "mixed_classified [\"event_start\"] = MBP_GFP_mixed['event_start']\n",
    "\n",
    "#Plot the decsion function\n",
    "##plt.figure(1)\n",
    "##plt.plot(dec_2[mixed_pred=='GFP10D'], marker=\"o\", c='g')\n",
    "##plt.plot(dec_2[mixed_pred=='MBP10D'], marker=\"o\", c='r')\n",
    "\n",
    "#Plot the decsion function with respect to probability score\n",
    "##plt.figure(2)\n",
    "##plt.scatter(dec_2[mixed_pred=='GFP10D'],probs_2[:,0][mixed_pred=='GFP10D'],c='g')\n",
    "##plt.scatter(dec_2[mixed_pred=='MBP10D'],probs_2[:,0][mixed_pred=='MBP10D'],c='r')\n",
    "\n",
    "#Plotting function for classifying events in a particular mixed experiment file\n",
    "plt.figure(3)\n",
    "barsGFP = []\n",
    "barsMBP = []\n",
    "x_pos=[]\n",
    "widths=[]\n",
    "\n",
    "einfo=[]\n",
    "for event in file.events:\n",
    "    #event.classification=None\n",
    "    event.classification=2 #Set events that did not get trained in the SVM to a black color\n",
    "    c=mixed_classified.loc[(mixed['event_start']==event.start) & \n",
    "                           (file.filename==mixed['file'])] #& \n",
    "    \n",
    "    if not c.empty:\n",
    "        x_pos.append(event.start+((event.end-event.start)/2))\n",
    "        widths.append(1)#(event.end-event.start)\n",
    "        barsGFP.append(c.iloc[0]['pGFP'])\n",
    "        barsMBP.append(c.iloc[0]['pMBP'])\n",
    "        \n",
    "        \n",
    "        if(c.iloc[0]['type']=='GFP10D'): \n",
    "            event.classification=0\n",
    "            einfo.append((int(event.second*(event.start-10e-4)),\n",
    "                      int(event.second*(event.end+10e-4)),event.classification)) \n",
    "                     \n",
    "        elif (c.iloc[0]['type']=='MBP10D'): \n",
    "            event.classification=1\n",
    "            einfo.append((int(event.second*(event.start-10e-4)),\n",
    "                      int(event.second*(event.end+10e-4)),event.classification)) \n",
    "                \n",
    "m_file = mixed_classified.loc[(mixed_classified['file']==file.filename)] \n",
    "m_file [\"x_pos\"] = x_pos\n",
    "\n",
    "\n",
    "####Function for probability y-axis tics###\n",
    "def update_ticks(y, pos):\n",
    "    if y == 0.0:\n",
    "        return '100% GFP'\n",
    "    elif y == 1.0:\n",
    "        return '100% MBP'\n",
    "    else:\n",
    "        return y\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "mpl.rcParams['lines.linewidth'] = 1.5\n",
    "\n",
    "###Making weight column for probability score###\n",
    "color_weights = []\n",
    "for index, row in m_file.iterrows():\n",
    "    #print(row['pMBP'])\n",
    "    color_weights.append(abs(row['pMBP']-0.5))\n",
    "m_file [\"color_weights\"] = color_weights\n",
    "\n",
    "#Plot colored events with probability scores associated with each event and gradient transparency associated with prob score\n",
    "# ax2=plt.subplot(gs[-1, :],sharex=ax1)\n",
    "# x = m_file.loc[m_file[\"type\"]=='GFP10D'][\"x_pos\"]\n",
    "# y = m_file.loc[m_file[\"type\"]=='GFP10D'][\"pMBP\"]\n",
    "# z =  m_file.loc[m_file[\"type\"]=='GFP10D'][\"color_weights\"]\n",
    "# plt.figure(figsize=(21,9),dpi=200)\n",
    "# plt.scatter(x, y, s=200, c=z, cmap='Greens', edgecolors='k')\n",
    "# x = m_file.loc[m_file[\"type\"]=='MBP10D'][\"x_pos\"]\n",
    "# y = m_file.loc[m_file[\"type\"]=='MBP10D'][\"pMBP\"]\n",
    "# z =  m_file.loc[m_file[\"type\"]=='MBP10D'][\"color_weights\"]\n",
    "# plt.scatter(x, y, s=200, c=z, cmap='Reds', edgecolors='k')\n",
    "# ax2.set(xlabel='Time [s]', ylabel='Decision Probability')\n",
    "# plt.yticks([0.0, 0.5, 1.0])\n",
    "# plt.axhline(y = 0.5, color = 'k', linestyle = 'dashed')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# plt.xlim(20, 145)\n",
    "# ax2.yaxis.set_major_formatter(mticker.FuncFormatter(update_ticks))\n",
    "\n",
    "#Plot colored events with probability scores associated with each event and gradient transparency associated with prob score\n",
    "plt.figure(figsize=(21,14),dpi=300)\n",
    "gs = gridspec.GridSpec(4,1) \n",
    "ax1=plt.subplot(gs[:3, :]) \n",
    "file.plot( limits=None, color_events=True, event_downsample=1, \n",
    "        file_downsample=10, downsample=10, file_kwargs={ 'c':'k', 'alpha':1},\n",
    "        event_kwargs={ 'c':'c', 'alpha':1 }, multiclass=True,classcolors=['g','r', 'black', 'purple'])\n",
    "ax1.set_xlabel('', fontsize = 20)\n",
    "ax1.set_ylabel('Ionic Current [nA]', fontsize = 20)\n",
    "#ax1.set(title= 'MBP and GFP Mixture GBC Model Classification',xlabel='Time [s]', ylabel='Ionic Current [nA]')    \n",
    "\n",
    "ax2=plt.subplot(gs[-1, :],sharex=ax1)\n",
    "x1 = m_file.loc[m_file[\"type\"]=='GFP10D'][\"x_pos\"]\n",
    "y1 = m_file.loc[m_file[\"type\"]=='GFP10D'][\"pMBP\"]\n",
    "z1 =  m_file.loc[m_file[\"type\"]=='GFP10D'][\"color_weights\"]\n",
    "plt.scatter(x1, y1, s=200, c=z1, cmap='Greens', edgecolors='k')\n",
    "x2 = m_file.loc[m_file[\"type\"]=='MBP10D'][\"x_pos\"]\n",
    "y2 = m_file.loc[m_file[\"type\"]=='MBP10D'][\"pMBP\"]\n",
    "z2 =  m_file.loc[m_file[\"type\"]=='MBP10D'][\"color_weights\"]\n",
    "plt.scatter(x2, y2, s=200, c=z2, cmap='Reds', edgecolors='k')\n",
    "#ax2.set(xlabel='Time [s]', ylabel='Decision Probability')\n",
    "ax2.set_xlabel('Time [s]', fontsize = 20)\n",
    "ax2.set_ylabel('Decision Probability', fontsize = 20)\n",
    "\n",
    "ax2.set_ylim([-0.1, 1.1])\n",
    "ax2.yaxis.set_major_formatter(mticker.FuncFormatter(update_ticks))\n",
    "plt.yticks([0.0, 0.5, 1.0])\n",
    "plt.axhline(y = 0.5, color = 'k', linestyle = 'dashed')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "#plt.xlim(15.5, 32.5) #80 to 20 limit\n",
    "#plt.xlim(1, 18) #20 to 80 limit\n",
    "plt.xlim(38, 55.5) #50 to 50 limit\n",
    "\n",
    "#plt.savefig('Unlabeled_Calls_Prob_50_50.png', dpi = 300)\n",
    "#plt.savefig('Unlabeled_Calls_Prob_50_50.svg', transparent=True, dpi = 300)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Plot the matrix with a heatmap colorbar\n",
    "plt.figure()\n",
    "data = np.random.rand(10,10) * 2 - 1\n",
    "colors1 = plt.cm.Greens(np.linspace(1., 0., 128, endpoint=True))\n",
    "colors2 = plt.cm.Reds(np.linspace(0., 1., 128, endpoint=True))\n",
    "#combine them and build a new colormap\n",
    "colors = np.vstack((colors1, colors2))\n",
    "mymap = mcolors.LinearSegmentedColormap.from_list('my_colormap', colors)\n",
    "\n",
    "v1 = np.linspace(0, 1.0, 5, endpoint=True)\n",
    "#plt.pcolor(data, cmap=mymap)\n",
    "cbar=plt.imshow(data, vmin=-1., vmax=1., cmap=mymap)\n",
    "cbar = plt.colorbar()\n",
    "#plt.savefig(\"Color_Map.svg\", transparent=True)\n",
    "\n",
    "\n",
    "\n",
    "###With seaborn###\n",
    "# ax2=plt.subplot(gs[-1, :],sharex=ax1)\n",
    "# sns.scatterplot(data=m_file, x=\"x_pos\", y=\"pMBP\", hue=\"type\", marker=\"o\", palette=['red','green'], s=120)\n",
    "# ax2.set(xlabel='Time [s]', ylabel='Decision Probability')\n",
    "# plt.yticks([0.0, 0.5, 1.0])\n",
    "# plt.axhline(y = 0.5, color = 'k', linestyle = 'dashed')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# plt.xlim(20, 145)\n",
    "# ax2.yaxis.set_major_formatter(mticker.FuncFormatter(update_ticks))\n",
    "# #plt.savefig(\"Probability_Scored_Events.svg\", transparent=True)\n",
    "\n",
    "\n",
    "#Only plots the colored events\n",
    "plt.figure(figsize=(1,1))\n",
    "file.plot( limits=None, color_events=True, event_downsample=1, \n",
    "        file_downsample=10, downsample=10, file_kwargs={ 'c':'k', 'alpha':1},\n",
    "        event_kwargs={ 'c':'c', 'alpha':1 }, multiclass=True, classcolors=['g','r', 'black', 'purple'])  \n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Ionic Current [nA]')\n",
    "plt.title('MBP and GFP Mixture SVM Classification')\n",
    "plt.xlim(20, 30)\n",
    "\n",
    "\n",
    "#print(x_pos)\n",
    "#print(einfo)\n",
    "#print(\"Here: \" + str(m_file[\"pMBP\"]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Plotting mixture calls with probability scores####\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "import matplotlib as mpl\n",
    "import joblib\n",
    "import matplotlib.ticker as mticker\n",
    "sns.set()\n",
    "sns.set_style(style='white')\n",
    "\n",
    "#%matplotlib qt5\n",
    "%matplotlib inline\n",
    "\n",
    "calls_dict = {'GFP10D': [], 'MBP10D': []}\n",
    "\n",
    "#############################################################################\n",
    "#filter_file = \"B071222\"\n",
    "\n",
    "#MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_20_80']\n",
    "#MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_40_60']\n",
    "MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_50_50']\n",
    "#MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_60_40']\n",
    "#MBP_GFP_mixed = nomixed.loc[nomixed['type']=='MBP+GFP10D_80_20']\n",
    "\n",
    "#MBP_GFP_mixed = MBP_GFP_mixed[MBP_GFP_mixed['file'].str.contains(filter_file)!=True]\n",
    "print(f\"Size of mixture dataframe we are predicting on: {len(MBP_GFP_mixed)}\")\n",
    "#############################################################################\n",
    "nomixed=eventsDf\n",
    "prior_NA = len(nomixed)\n",
    "\n",
    "nomixed=nomixed.loc[nomixed['duration']>300e-6].loc[nomixed['duration']<20e-3] #minimum of 125 samples and maximum of 5000 samples\n",
    "#nomixed=nomixed.loc[nomixed['duration']>900e-6]\n",
    "\n",
    "post_NA = len(nomixed)\n",
    "removed_NA_df = nomixed.dropna()\n",
    "\n",
    "print(f\"Size of dataframe prior to filtering by duration: {prior_NA}\")\n",
    "print(f\"Size of dataframe after filtering by duration: {post_NA}\")\n",
    "print(f\"Size of dataframe after  filtering by duration and dropping NaNs from dataframe: {len(removed_NA_df)}\")\n",
    "\n",
    "updated_df_list = []\n",
    "type_list = [\"MBP10D\", \"GFP10D\"]\n",
    "#type_list = [\"MBP10D\", \"MBPMBP10D\"]\n",
    "#type_list = [\"MBP10D\", \"MBP Nterm\"]\n",
    "#type_list = [\"MBP Nterm\", \"MBPMBP10D\"]\n",
    "#type_list = [\"MBP10D\", \"MBP Nterm\", \"MBPMBP10D\"]\n",
    "\n",
    "#Labels = [\"MBPD10\", \"D10MBP\", \"diMBPD10\"]\n",
    "Labels = [\"MBPD10\", \"GFPD10\"]\n",
    "\n",
    "for label in type_list:\n",
    "    df = nomixed.loc[nomixed[\"type\"] == label]\n",
    "    updated_df_list.append(df)\n",
    "noMixed = pd.concat(updated_df_list)\n",
    "noMixed['type'].value_counts()\n",
    "limiting_sample_size = min(noMixed['type'].value_counts())\n",
    "\n",
    "print(f\"Sample size of each class for training and testing: {limiting_sample_size}\")\n",
    "seedVal_sample = random.randint(0, 200)\n",
    "\n",
    "updated_df_list = []\n",
    "for label in type_list:\n",
    "    df = noMixed.loc[noMixed[\"type\"]==label].sample(limiting_sample_size, random_state=seedVal_sample)\n",
    "    updated_df_list.append(df)\n",
    "noMixed = pd.concat(updated_df_list)\n",
    "    #noMixed['type'].value_counts()\n",
    "print(f\"Total size of training and testing data: {noMixed['type'].value_counts()}\")\n",
    "\n",
    "\n",
    "x=noMixed.drop(columns=['event','file','I_0','Imax', 'mean', 'std', 'event_start', 'event_end'])#, 'fc_0'])\n",
    "x=x.drop(columns=['type','Blockage','mean/I_0', 'duration', 'std/I_0', 'Imin', 'max/std', 'min/duration', 'duration/std'])#'Imax/I_0','min/max','Imin/I_0','min/mean','min/std', 'mean/std'])\n",
    "x=x.drop(columns=['min/max', 'min/mean', 'min/std', 'mean/std', 'Imin/I_0', 'Imax/I_0'])#'Imax/I_0','min/max','Imin/I_0','min/mean','min/std', 'mean/std'])\n",
    "\n",
    "print(f\"Feature space for training model: {feature_space}\")\n",
    "x = x.loc[:, feature_space]\n",
    "y=noMixed['type']\n",
    "seedVal_trte_split = random.randint(0, 200)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=seedVal_trte_split) #2 #40 (123 seed was good for all four analytes)\n",
    "scaler = sk.preprocessing.StandardScaler()\n",
    "x_train = scaler.fit_transform( x_train )\n",
    "x_test = scaler.transform( x_test )\n",
    "    \n",
    "print(f\"Expected: {y_test.value_counts()}\")\n",
    "        \n",
    "gbc = GradientBoostingClassifier(n_estimators=200,max_depth=5)\n",
    "#rfc = RandomForestClassifier(n_estimators=200, max_depth=5)#, max_features=None)\n",
    "#logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "#knn = KNeighborsClassifier(n_neighbors=5)\n",
    "#clf = SVC(decision_function_shape='ovr',probability=True,kernel='rbf', \n",
    "           #C=200,verbose=True,gamma='scale', class_weight = None)\n",
    "\n",
    "gbc.fit(x_train, y_train)\n",
    "\n",
    "y_pred=gbc.predict(x_test)\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "print(f\"Predicted: {dict(zip(unique, counts))}\")\n",
    "\n",
    "classification_accuracy.append(accuracy_score(y_test,y_pred)*100)\n",
    "print('Mean classification accuracy of GBC model: {}'.format(np.mean(classification_accuracy)))  \n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "mixed=MBP_GFP_mixed.drop(columns=['event','file','I_0','Imax', 'mean', 'std', 'event_start', 'event_end'])#, 'fc_0'])\n",
    "\n",
    "#More features are removed\n",
    "mixed = mixed.loc[:, feature_space]\n",
    "\n",
    "mixed_scaled = scaler.transform(mixed)\n",
    "mixed_pred = gbc.predict(mixed_scaled)\n",
    "#mixed_exp_calls = gbc.predict(mixed)\n",
    "#mixed_exp_calls = clf.predict(mixed_scaled)\n",
    "unique, counts = np.unique(mixed_exp_calls, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "probability_scores = gbc.predict_proba(mixed_scaled)\n",
    "##Probs = pd.DataFrame(probs_2, columns = ['GFP10D', 'MBP10D'])\n",
    "Probs = pd.DataFrame(probability_scores, columns = ['GFP10D', 'MBP10D'])\n",
    "print(Probs['GFP10D'])\n",
    "print(Probs['MBP10D'])\n",
    "\n",
    "\n",
    "\n",
    "#print(probability_scores)\n",
    "#probs_2 = clf_2.predict_proba(X_norm_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Prepare mixture dataset pandas dataframe\n",
    "#mixed=eventsDf.loc[eventsDf['type']!=\"GFP10D\"].loc[eventsDf['type']!=\"MBP10D\"].loc[eventsDf['type']!=\"MBP Nterm\"].loc[eventsDf['type']!=\"MBPMBP10D\"].loc[eventsDf['type']!=\"GFP Dye 10D\"]\n",
    "##mixed=eventsDf.loc[eventsDf['type']!=\"GFP10D\"].loc[eventsDf['type']!=\"MBP10D\"]\n",
    "##mixed=mixed.loc[mixed['duration']>500e-6].loc[mixed['duration']<20e-3]\n",
    "##print(mixed['type'].value_counts())\n",
    "\n",
    "#Remove features that were not used for SVM training, keeping the vectors of interest\n",
    "##x_2=mixed.drop(columns=['type','event','file','I_0','Imax', 'mean', 'std', 'event_start', 'event_end'])\n",
    "##x_2=x_2.drop(columns=['Blockage','mean/I_0', 'duration', 'std/I_0', 'Imin', 'max/std', 'min/duration', 'duration/std'])\n",
    "\n",
    "\n",
    "#Normalize the data associated with each vector column-wise\n",
    "#x_norm_2 = sk.preprocessing.StandardScaler().fit_transform(x_2)\n",
    "#x_norm_2 = sk.preprocessing.MinMaxScaler(feature_range = (-1, 1)).fit_transform(x_2)\n",
    "#X_norm_2 = pd.DataFrame(x_norm_2)\n",
    "\n",
    "# scaler = sk.preprocessing.StandardScaler()\n",
    "# x_norm_2 = scaler.fit_transform( x_2 )\n",
    "# X_norm_2 = pd.DataFrame(x_norm_2)\n",
    "\n",
    "##X_norm_2 = scaler.transform( x_2 )\n",
    "#save trained model\n",
    "##joblib.dump(clf, \"MBP_GFP_Model.pkl\") \n",
    "\n",
    "#load trained model and test on unlabeled mixture\n",
    "##clf_2 = joblib.load(\"MBP_GFP_Model.pkl\")\n",
    "\n",
    "#Extract probability and decision function output from model predictions\n",
    "##mixed_pred = clf_2.predict(X_norm_2)\n",
    "#Let's observe the how many events were classified as GFP and MBP in the mixed experiments\n",
    "##unique, counts = np.unique(mixed_pred, return_counts=True)\n",
    "##print(dict(zip(unique, counts)))\n",
    "##probs_2 = clf_2.predict_proba(X_norm_2)\n",
    "##dec_2 = clf_2.decision_function(X_norm_2)\n",
    "\n",
    "\n",
    "\n",
    "#Set up dataframe containing the probability confidence scores \n",
    "##Probs = pd.DataFrame(probability_scores, columns = ['GFP10D', 'MBP10D'])\n",
    "#print(Probs['GFP10D'])\n",
    "#print(Probs['MBP10D'])\n",
    "\n",
    "#Load one of the mixed sample files for event classification\n",
    "##file = experiments[\"MBP+GFP10D\"][1]\n",
    "\n",
    "#####################################################################################################\n",
    "#file = experiments[\"MBP+GFP10D_80_20\"][10]\n",
    "#file = experiments[\"MBP+GFP10D_20_80\"][0]\n",
    "#file = experiments[\"MBP+GFP10D_50_50\"][25]\n",
    "file = experiments[\"MBP+GFP10D_50_50\"][28]\n",
    "#####################################################################################################\n",
    "\n",
    "#Let's make a new dataframe that now has the classifications associated with each event\n",
    "mixed_classified = mixed\n",
    "mixed_classified ['type'] = mixed_pred\n",
    "mixed_classified [\"pGFP\"] = probability_scores[:,0]\n",
    "mixed_classified [\"pMBP\"] = probability_scores[:,1]\n",
    "mixed_classified [\"file\"] = MBP_GFP_mixed['file']\n",
    "mixed_classified [\"event_start\"] = MBP_GFP_mixed['event_start']\n",
    "\n",
    "#Plot the decsion function\n",
    "##plt.figure(1)\n",
    "##plt.plot(dec_2[mixed_pred=='GFP10D'], marker=\"o\", c='g')\n",
    "##plt.plot(dec_2[mixed_pred=='MBP10D'], marker=\"o\", c='r')\n",
    "\n",
    "#Plot the decsion function with respect to probability score\n",
    "##plt.figure(2)\n",
    "##plt.scatter(dec_2[mixed_pred=='GFP10D'],probs_2[:,0][mixed_pred=='GFP10D'],c='g')\n",
    "##plt.scatter(dec_2[mixed_pred=='MBP10D'],probs_2[:,0][mixed_pred=='MBP10D'],c='r')\n",
    "\n",
    "#Plotting function for classifying events in a particular mixed experiment file\n",
    "plt.figure(3)\n",
    "barsGFP = []\n",
    "barsMBP = []\n",
    "x_pos=[]\n",
    "widths=[]\n",
    "\n",
    "einfo=[]\n",
    "for event in file.events:\n",
    "    #event.classification=None\n",
    "    event.classification=2 #Set events that did not get trained in the SVM to a black color\n",
    "    c=mixed_classified.loc[(mixed['event_start']==event.start) & \n",
    "                           (file.filename==mixed['file'])] #& \n",
    "    \n",
    "    if not c.empty:\n",
    "        x_pos.append(event.start+((event.end-event.start)/2))\n",
    "        widths.append(1)#(event.end-event.start)\n",
    "        barsGFP.append(c.iloc[0]['pGFP'])\n",
    "        barsMBP.append(c.iloc[0]['pMBP'])\n",
    "        \n",
    "        \n",
    "        if(c.iloc[0]['type']=='GFP10D'): \n",
    "            event.classification=0\n",
    "            einfo.append((int(event.second*(event.start-10e-4)),\n",
    "                      int(event.second*(event.end+10e-4)),event.classification)) \n",
    "                     \n",
    "        elif (c.iloc[0]['type']=='MBP10D'): \n",
    "            event.classification=1\n",
    "            einfo.append((int(event.second*(event.start-10e-4)),\n",
    "                      int(event.second*(event.end+10e-4)),event.classification)) \n",
    "                \n",
    "m_file = mixed_classified.loc[(mixed_classified['file']==file.filename)] \n",
    "m_file [\"x_pos\"] = x_pos\n",
    "\n",
    "\n",
    "####Function for probability y-axis tics###\n",
    "def update_ticks(y, pos):\n",
    "    if y == 0.0:\n",
    "        return '100% GFP'\n",
    "    elif y == 1.0:\n",
    "        return '100% MBP'\n",
    "    else:\n",
    "        return y\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "mpl.rcParams['lines.linewidth'] = 1.5\n",
    "\n",
    "###Making weight column for probability score###\n",
    "color_weights = []\n",
    "for index, row in m_file.iterrows():\n",
    "    #print(row['pMBP'])\n",
    "    color_weights.append(abs(row['pMBP']-0.5))\n",
    "m_file [\"color_weights\"] = color_weights\n",
    "\n",
    "#Plot colored events with probability scores associated with each event and gradient transparency associated with prob score\n",
    "# ax2=plt.subplot(gs[-1, :],sharex=ax1)\n",
    "# x = m_file.loc[m_file[\"type\"]=='GFP10D'][\"x_pos\"]\n",
    "# y = m_file.loc[m_file[\"type\"]=='GFP10D'][\"pMBP\"]\n",
    "# z =  m_file.loc[m_file[\"type\"]=='GFP10D'][\"color_weights\"]\n",
    "# plt.figure(figsize=(21,9),dpi=200)\n",
    "# plt.scatter(x, y, s=200, c=z, cmap='Greens', edgecolors='k')\n",
    "# x = m_file.loc[m_file[\"type\"]=='MBP10D'][\"x_pos\"]\n",
    "# y = m_file.loc[m_file[\"type\"]=='MBP10D'][\"pMBP\"]\n",
    "# z =  m_file.loc[m_file[\"type\"]=='MBP10D'][\"color_weights\"]\n",
    "# plt.scatter(x, y, s=200, c=z, cmap='Reds', edgecolors='k')\n",
    "# ax2.set(xlabel='Time [s]', ylabel='Decision Probability')\n",
    "# plt.yticks([0.0, 0.5, 1.0])\n",
    "# plt.axhline(y = 0.5, color = 'k', linestyle = 'dashed')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# plt.xlim(20, 145)\n",
    "# ax2.yaxis.set_major_formatter(mticker.FuncFormatter(update_ticks))\n",
    "\n",
    "#Plot colored events with probability scores associated with each event and gradient transparency associated with prob score\n",
    "plt.figure(figsize=(21,14),dpi=300)\n",
    "gs = gridspec.GridSpec(4,1) \n",
    "ax1=plt.subplot(gs[:3, :]) \n",
    "file.plot( limits=None, color_events=True, event_downsample=1, \n",
    "        file_downsample=10, downsample=10, file_kwargs={ 'c':'k', 'alpha':1},\n",
    "        event_kwargs={ 'c':'c', 'alpha':1 }, multiclass=True,classcolors=['g','r', 'black', 'purple'])\n",
    "ax1.set_xlabel('', fontsize = 20)\n",
    "ax1.set_ylabel('Ionic Current [nA]', fontsize = 20)\n",
    "#ax1.set(title= 'MBP and GFP Mixture GBC Model Classification',xlabel='Time [s]', ylabel='Ionic Current [nA]')    \n",
    "\n",
    "ax2=plt.subplot(gs[-1, :],sharex=ax1)\n",
    "x1 = m_file.loc[m_file[\"type\"]=='GFP10D'][\"x_pos\"]\n",
    "y1 = m_file.loc[m_file[\"type\"]=='GFP10D'][\"pMBP\"]\n",
    "z1 =  m_file.loc[m_file[\"type\"]=='GFP10D'][\"color_weights\"]\n",
    "plt.scatter(x1, y1, s=200, c=z1, cmap='Greens', edgecolors='k')\n",
    "x2 = m_file.loc[m_file[\"type\"]=='MBP10D'][\"x_pos\"]\n",
    "y2 = m_file.loc[m_file[\"type\"]=='MBP10D'][\"pMBP\"]\n",
    "z2 =  m_file.loc[m_file[\"type\"]=='MBP10D'][\"color_weights\"]\n",
    "plt.scatter(x2, y2, s=200, c=z2, cmap='Reds', edgecolors='k')\n",
    "#ax2.set(xlabel='Time [s]', ylabel='Decision Probability')\n",
    "ax2.set_xlabel('Time [s]', fontsize = 20)\n",
    "ax2.set_ylabel('Decision Probability', fontsize = 20)\n",
    "\n",
    "ax2.set_ylim([-0.1, 1.1])\n",
    "ax2.yaxis.set_major_formatter(mticker.FuncFormatter(update_ticks))\n",
    "plt.yticks([0.0, 0.5, 1.0])\n",
    "plt.axhline(y = 0.5, color = 'k', linestyle = 'dashed')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "#plt.xlim(15.5, 32.5) #80 to 20 limit\n",
    "#plt.xlim(1, 18) #20 to 80 limit\n",
    "plt.xlim(38, 55.5) #50 to 50 limit\n",
    "\n",
    "#plt.savefig('Unlabeled_Calls_Prob_50_50.png', dpi = 300)\n",
    "#plt.savefig('Unlabeled_Calls_Prob_50_50.svg', transparent=True, dpi = 300)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Plot the matrix with a heatmap colorbar\n",
    "plt.figure()\n",
    "data = np.random.rand(10,10) * 2 - 1\n",
    "colors1 = plt.cm.Greens(np.linspace(1., 0., 128, endpoint=True))\n",
    "colors2 = plt.cm.Reds(np.linspace(0., 1., 128, endpoint=True))\n",
    "#combine them and build a new colormap\n",
    "colors = np.vstack((colors1, colors2))\n",
    "mymap = mcolors.LinearSegmentedColormap.from_list('my_colormap', colors)\n",
    "\n",
    "v1 = np.linspace(0, 1.0, 5, endpoint=True)\n",
    "#plt.pcolor(data, cmap=mymap)\n",
    "cbar=plt.imshow(data, vmin=-1., vmax=1., cmap=mymap)\n",
    "cbar = plt.colorbar()\n",
    "#plt.savefig(\"Color_Map.svg\", transparent=True)\n",
    "\n",
    "\n",
    "\n",
    "###With seaborn###\n",
    "# ax2=plt.subplot(gs[-1, :],sharex=ax1)\n",
    "# sns.scatterplot(data=m_file, x=\"x_pos\", y=\"pMBP\", hue=\"type\", marker=\"o\", palette=['red','green'], s=120)\n",
    "# ax2.set(xlabel='Time [s]', ylabel='Decision Probability')\n",
    "# plt.yticks([0.0, 0.5, 1.0])\n",
    "# plt.axhline(y = 0.5, color = 'k', linestyle = 'dashed')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# plt.xlim(20, 145)\n",
    "# ax2.yaxis.set_major_formatter(mticker.FuncFormatter(update_ticks))\n",
    "# #plt.savefig(\"Probability_Scored_Events.svg\", transparent=True)\n",
    "\n",
    "\n",
    "#Only plots the colored events\n",
    "plt.figure(figsize=(1,1))\n",
    "file.plot( limits=None, color_events=True, event_downsample=1, \n",
    "        file_downsample=10, downsample=10, file_kwargs={ 'c':'k', 'alpha':1},\n",
    "        event_kwargs={ 'c':'c', 'alpha':1 }, multiclass=True, classcolors=['g','r', 'black', 'purple'])  \n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Ionic Current [nA]')\n",
    "plt.title('MBP and GFP Mixture SVM Classification')\n",
    "plt.xlim(20, 30)\n",
    "\n",
    "\n",
    "#print(x_pos)\n",
    "#print(einfo)\n",
    "#print(\"Here: \" + str(m_file[\"pMBP\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabbing and displaying 20 events, 10 that were called as GFPD10 and 10 as MBPD10\n",
    "current=file.current\n",
    "\n",
    "\n",
    "gfp=[e for e in einfo if e[2]==0]\n",
    "mbp=[e for e in einfo if e[2]==1]\n",
    "step=file.second\n",
    "laststart = 0\n",
    "gap=250\n",
    "\n",
    "plt.figure(figsize=(20,12),dpi=200)\n",
    "ax=plt.subplot(211)\n",
    "plt.ylim(-50,400)\n",
    "plt.title(\"Unlabeled MBPD10 Calls\")\n",
    "plt.ylabel(\"I [pA]\")\n",
    "for (start,end,_) in mbp[55:65]:\n",
    "    print(start, end, _)\n",
    "    thisend=laststart+(end-start)\n",
    "    plt.plot (np.arange(laststart,thisend)/step,1000*current[start:end],linewidth=0.6, c='r', alpha=1)\n",
    "    laststart=thisend+gap\n",
    "\n",
    "laststart=0\n",
    "plt.subplot(212,sharex=ax)\n",
    "plt.ylim(-50,400)\n",
    "plt.title (\"Unlabeled GFPD10 Calls\")\n",
    "plt.ylabel(\"I [pA]\")\n",
    "for (start,end,_) in gfp[25:42]:\n",
    "    thisend=laststart+(end-start)\n",
    "    plt.plot (np.arange(laststart,thisend)/step,1000*current[start:end],linewidth=0.6, c='g', alpha=1)\n",
    "    laststart=thisend+gap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
